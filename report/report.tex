\documentclass[conference]{IEEEtran}

% ===== Packages (kept minimal and IEEE-friendly) =====
\usepackage{amsmath,amssymb,bm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[caption=false,font=footnotesize]{subfig}
\usepackage{siunitx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{cite}
\usepackage{microtype}

% ===== Useful macros to enforce notation rules =====
\newcommand{\vect}[1]{\mathbf{#1}} % vectors bold, lowercase
\newcommand{\matr}[1]{\mathbf{#1}} % matrices bold, uppercase
\newcommand{\set}[1]{\mathrm{#1}}  % sets upper case, not bold
\newcommand{\func}[1]{\mathrm{#1}} % functions lower case, not bold

% ===== Student metadata =====
\newcommand{\studentnumber}{25935410}
\newcommand{\modcode}{RW441}
\newcommand{\surname}{Genders}
\newcommand{\nameinit}{D. A.}
\newcommand{\emailaddr}{25935410@sun.ac.za}

% ===== Title =====
\title{Active Learning with Neural Networks: Uncertainty Sampling and Sensitivity Analysis}

\author{\IEEEauthorblockN{\nameinit\ \surname\ \, (\studentnumber)}
\IEEEauthorblockA{Stellenbosch University\\ Machine Learning 441\\ \emailaddr}}

\begin{document}
\maketitle

\begin{abstract}
Active learning aims to reduce labeling costs by intelligently selecting the most informative samples for annotation. This study investigates two families of query strategies for neural networks: uncertainty sampling and sensitivity-based selection. The study implements uncertainty sampling using classical formulations including least confidence, margin, and entropy, and proposes a sensitivity-based approach using output Jacobian norms. Experiments are conducted on six datasets spanning classification tasks including Iris, Wine, and Breast Cancer, and regression tasks including Diabetes, Linnerud, and California Housing, with varying complexity. A one-hidden-layer multilayer perceptron is trained using minibatch stochastic gradient descent (SGD) with weight decay and early stopping. The empirical protocol employs hyperparameter optimization via randomized search, multiple random seeds, and rigorous statistical evaluation. Performance is measured using standard metrics including accuracy, F1-macro, and area under the receiver operating characteristic curve for classification, and root mean squared error, mean absolute error, and coefficient of determination for regression, together with active learning-specific metrics including label efficiency and learning curves. Results demonstrate that sensitivity-based selection consistently outperformed uncertainty methods on complex problems, achieving superior label efficiency especially at smaller budgets. Uncertainty sampling shows competitive performance on simpler tasks, with entropy and margin methods performing similarly. The findings indicate that sensitivity analysis is particularly effective for high-dimensional, complex problems where traditional uncertainty measures are less informative.
\end{abstract}

\section{Introduction}

Machine learning models often require large amounts of labeled data to achieve satisfactory performance, but obtaining high-quality labels can be expensive and time-consuming. Active learning addresses this challenge by intelligently selecting the most informative unlabeled samples for annotation, thereby reducing labeling costs while maintaining model performance. This approach is particularly valuable in domains where labeling is costly, such as medical diagnosis, scientific annotation, or expert knowledge acquisition.

Traditional active learning strategies rely on uncertainty sampling, which selects samples where the model is least certain about its predictions \cite{settles2009active}. These methods include least confidence, margin sampling, and entropy-based selection, all of which leverage the probabilistic outputs of machine learning models. However, uncertainty-based approaches do not always capture the most informative samples, especially in complex, high-dimensional problems where the relationship between inputs and outputs is non-linear.

This study investigates an alternative approach based on sensitivity analysis, which selects samples with high output sensitivity to input perturbations. By computing the Jacobian norm of model outputs with respect to inputs, the study identifies samples where small changes in input features lead to significant changes in predictions. This sensitivity-based selection is particularly effective for neural networks, where the learned representations capture complex input-output relationships.

The primary goal of this research is to compare uncertainty sampling and sensitivity-based selection strategies for active learning with neural networks. The study aims to understand three key questions: first, which approach provides better label efficiency across different problem complexities; second, how performance varies with dataset characteristics; and third, whether sensitivity analysis offers advantages over traditional uncertainty methods.

To achieve these goals, the study implements both uncertainty sampling variants and a novel sensitivity-based strategy using output Jacobian norms. The study conducts comprehensive experiments on six datasets spanning classification and regression tasks of varying complexity, from simple tabular datasets to more challenging high-dimensional problems. The empirical protocol employs rigorous hyperparameter optimization, multiple random seeds, and statistical evaluation to ensure reliable and reproducible results.

The main contributions of this work include four key elements: first, a novel sensitivity-based active learning strategy using output Jacobian norms; second, a comprehensive comparison of uncertainty and sensitivity approaches across multiple datasets and tasks; third, a principled empirical protocol with rigorous statistical evaluation; and fourth, insights into when each approach is most effective.

The results demonstrate that sensitivity-based selection consistently outperformed uncertainty methods on complex problems, achieving superior label efficiency especially at smaller budgets. Uncertainty sampling shows competitive performance on simpler tasks, with entropy and margin methods performing similarly. These findings indicate that sensitivity analysis is particularly effective for high-dimensional, complex problems where traditional uncertainty measures are less informative.

The remainder of this report is organized as follows: Section II provided background on neural networks, uncertainty sampling, and sensitivity analysis; Section III details the methodology and implementation; Section IV describes the empirical procedure including datasets, hyperparameters, and evaluation metrics; Section V presents and analyzes the experimental results; and Section VI concludes with key findings and future directions.

\section{Background}

\subsection{Neural Networks and Supervised Learning}

Neural networks are powerful function approximators that learn complex mappings from inputs to outputs through iterative optimization \cite{gal2017deep}. A feedforward neural network consists of multiple layers of interconnected neurons, where each neuron applies a non-linear activation function to a weighted sum of its inputs. The network learns by minimizing a differentiable loss function using backpropagation and stochastic gradient descent (SGD).

For classification tasks, the output layer typically uses a softmax activation to produce probability distributions over classes, and the cross-entropy loss is minimized. For regression tasks, a linear output layer produces continuous values, and the mean squared error loss is commonly used. Regularization techniques such as weight decay, also known as L2 regularization, and early stopping help prevent overfitting by controlling model complexity and stopping training when validation performance plateaus.

The success of neural networks depends heavily on the availability of large amounts of labeled training data. However, obtaining high-quality labels can be expensive and time-consuming, particularly in domains requiring expert knowledge or specialized equipment.

\subsection{Active Learning}

Active learning is a machine learning paradigm that aims to reduce labeling costs by intelligently selecting the most informative unlabeled samples for annotation \cite{sener2018active}. The key idea is that not all samples are equally valuable for improving model performance; some samples provide more information about the underlying data distribution than others.

The active learning process typically follows an iterative cycle with five steps: first, train a model on the current labeled set; second, use the trained model to score unlabeled samples; third, select the most informative samples for labeling; fourth, add the newly labeled samples to the training set; and fifth, repeat until a labeling budget is exhausted or performance converges.

The effectiveness of active learning depends critically on the query strategy used to select samples. A good query strategy should identify samples that, when labeled and added to the training set, will most improve model performance.

\subsection{Uncertainty Sampling}

Uncertainty sampling is one of the most widely used active learning strategies. The core idea is to select samples where the model is least certain about its predictions, as these samples are likely to provide the most information when labeled.

For classification tasks, uncertainty sampling can be implemented using several criteria:

\textbf{Least Confidence:} Select samples with the lowest maximum class probability:
\begin{equation}
x^* = \arg\min_{x \in U} \max_c p(c \mid x)
\end{equation}

\textbf{Margin Sampling:} Select samples with the smallest margin between the two most probable classes:
\begin{equation}
x^* = \arg\min_{x \in U} (p_{(1)} - p_{(2)})
\end{equation}
where $p_{(1)}$ and $p_{(2)}$ are the first and second highest class probabilities.

\textbf{Entropy:} Select samples with the highest prediction entropy:
\begin{equation}
x^* = \arg\max_{x \in U} -\sum_{c=1}^C p(c \mid x) \log p(c \mid x)
\end{equation}

These criteria are based on the assumption that samples with high uncertainty are more informative for improving model performance. However, uncertainty sampling is not always optimal, especially when the model's uncertainty estimates are unreliable or when the relationship between inputs and outputs is complex.

\subsection{Sensitivity Analysis}

Sensitivity analysis is a technique for understanding how sensitive a model's outputs are to changes in its inputs. In the context of active learning, sensitivity-based selection aims to identify samples where small changes in input features lead to significant changes in model predictions.

For a neural network $f: \mathbb{R}^d \rightarrow \mathbb{R}^k$, the sensitivity of output $j$ to input $i$ is given by the partial derivative $\frac{\partial f_j(x)}{\partial x_i}$. The overall sensitivity of a sample can be measured using the Frobenius norm of the Jacobian matrix:

\begin{equation}
\text{Sensitivity}(x) = \left\|\frac{\partial f(x)}{\partial x}\right\|_F = \sqrt{\sum_{i=1}^d \sum_{j=1}^k \left(\frac{\partial f_j(x)}{\partial x_i}\right)^2}
\end{equation}

Sensitivity-based selection queries samples with the highest sensitivity scores:
\begin{equation}
x^* = \arg\max_{x \in U} \text{Sensitivity}(x)
\end{equation}

The intuition behind sensitivity-based selection is that samples with high sensitivity represent regions of the input space where the model's predictions are most sensitive to input variations. These regions correspond to decision boundaries or areas where the model has learned complex, non-linear relationships between inputs and outputs.

Sensitivity analysis has been used in various contexts, including feature selection, model interpretation, and robustness analysis. In active learning, sensitivity-based selection is particularly effective for neural networks, where the learned representations capture complex input-output relationships that are not well-captured by traditional uncertainty measures.

\section{Methodology}

\subsection{Neural Network Architecture}

The study employs a one-hidden-layer multilayer perceptron for both classification and regression tasks. The network architecture consists of:

\begin{itemize}
\item \textbf{Input layer:} Accepts feature vectors of dimension $d$, which varies by dataset
\item \textbf{Hidden layer:} Fully connected layer with $h$ hidden units and ReLU activation
\item \textbf{Output layer:} 
  \begin{itemize}
  \item For classification: Linear layer with $C$ outputs, where $C$ is the number of classes, followed by softmax activation
  \item For regression: Linear layer with a single output and no activation
  \end{itemize}
\end{itemize}

The network parameters are initialized using Kaiming initialization for the linear layers, which helps with gradient flow during training. The model is trained using minibatch stochastic gradient descent (SGD) with the following components:

\begin{itemize}
\item \textbf{Loss functions:} Cross-entropy for classification, mean squared error for regression
\item \textbf{Regularization:} Weight decay, also known as L2 regularization, to prevent overfitting
\item \textbf{Early stopping:} Training stops when validation loss plateaus to prevent overfitting
\end{itemize}

\subsection{Active Learning Framework}

The active learning implementation follows a standard iterative procedure:

\begin{enumerate}
\item \textbf{Initialization:} Start with a small labeled set $L_0$ and a large unlabeled pool $U$
\item \textbf{Training:} Train the neural network on the current labeled set $L$ until convergence
\item \textbf{Scoring:} Use the trained model to score all samples in the unlabeled pool $U$
\item \textbf{Selection:} Select the top-$b$ samples with highest scores for labeling
\item \textbf{Update:} Add the newly labeled samples to $L$ and remove them from $U$
\item \textbf{Repeat:} Continue until the labeling budget is exhausted
\end{enumerate}

The batch size $b$ is fixed for each iteration, and the total labeling budget determines how many iterations are performed.

\subsection{Query Strategies}

\subsubsection{Uncertainty Sampling}

For classification tasks, the study implements three uncertainty sampling variants:

\textbf{Least Confidence:} Selects samples with the lowest maximum class probability:
\begin{equation}
\text{Score}(x) = 1 - \max_c p(c \mid x)
\end{equation}

\textbf{Margin Sampling:} Selects samples with the smallest margin between the two most probable classes:
\begin{equation}
\text{Score}(x) = p_{(1)} - p_{(2)}
\end{equation}

\textbf{Entropy:} Selects samples with the highest prediction entropy:
\begin{equation}
\text{Score}(x) = -\sum_{c=1}^C p(c \mid x) \log p(c \mid x)
\end{equation}

For regression tasks, the study uses a magnitude-based proxy for uncertainty since prediction probabilities cannot be directly computed. The study selects samples with the highest absolute prediction values, assuming that samples with larger predictions are more informative.

\subsubsection{Sensitivity-Based Selection}

The sensitivity-based approach computes the Jacobian norm of model outputs with respect to inputs. For a neural network $f: \mathbb{R}^d \rightarrow \mathbb{R}^k$, the sensitivity score is:

\begin{equation}
\text{Sensitivity}(x) = \left\|\frac{\partial f(x)}{\partial x}\right\|_F = \sqrt{\sum_{i=1}^d \sum_{j=1}^k \left(\frac{\partial f_j(x)}{\partial x_i}\right)^2}
\end{equation}

For classification tasks with $k$ classes, we aggregate the sensitivity across all output dimensions. For regression tasks with a single output, the sensitivity reduces to:

\begin{equation}
\text{Sensitivity}(x) = \sqrt{\sum_{i=1}^d \left(\frac{\partial f(x)}{\partial x_i}\right)^2}
\end{equation}

The Jacobian is computed using automatic differentiation, which allows for efficient computation of gradients with respect to inputs.

\subsection{Implementation Details}

The active learning framework is implemented in Python using PyTorch for neural network operations and automatic differentiation. Key implementation details include:

\begin{itemize}
\item \textbf{Gradient computation:} Jacobian matrices are computed using PyTorch's automatic differentiation capabilities
\item \textbf{Batch processing:} Sensitivity scores are computed in batches for efficiency
\item \textbf{Memory management:} Unlabeled samples are processed in chunks to manage memory usage
\item \textbf{Reproducibility:} Random seeds are fixed for consistent results across runs
\end{itemize}

\begin{algorithm}[t]
\caption{Active Learning with Uncertainty or Sensitivity}
\label{alg:al}
\begin{algorithmic}[1]
\State Initialize labeled set $L$ with $n_0$ examples; unlabeled pool $U$.
\While{budget not exhausted}
  \State Train network on $L$ with early stopping.
  \State Score each $x\in U$ with uncertainty (LC/margin/entropy) or sensitivity (Jacobian norm).
  \State Select top-$b$ examples $S\subset U$; query labels; update $L\leftarrow L\cup S$, $U\leftarrow U\setminus S$.
\EndWhile
\end{algorithmic}
\end{algorithm}

\section{Empirical Procedure}

\subsection{Datasets}

The study evaluated active learning strategies on six datasets spanning classification and regression tasks of varying complexity:

\textbf{Classification Datasets:}
\begin{itemize}
\item \textbf{Iris:} 150 samples, 4 features, 3 classes with low complexity
\item \textbf{Wine:} 178 samples, 13 features, 3 classes with moderate complexity
\item \textbf{Breast Cancer:} 569 samples, 30 features, 2 classes with higher complexity
\end{itemize}

\textbf{Regression Datasets:}
\begin{itemize}
\item \textbf{Diabetes:} 442 samples, 10 features with low-moderate complexity
\item \textbf{Linnerud:} 20 samples, 3 features with moderate complexity
\item \textbf{California Housing:} 20,640 samples, 8 features with higher complexity
\end{itemize}

All features were standardized using training set statistics including mean and standard deviation. For small datasets including Iris, Wine, and Linnerud, the study used cross-validation for hyperparameter selection. For larger datasets, the study used a train/validation/test split.

\subsection{Hyperparameter Configuration}

The study employed randomized search for hyperparameter optimization with the following search spaces:

\begin{itemize}
\item \textbf{Learning rate:} $\{0.003, 0.01, 0.03\}$
\item \textbf{Weight decay:} $\{0, 10^{-5}, 10^{-4}\}$
\item \textbf{Batch size:} $\{32, 64, 128\}$
\item \textbf{Hidden units:} $\{32, 64, 128\}$
\item \textbf{Early stopping patience:} 10, 20, or 30 epochs
\end{itemize}

For each hyperparameter configuration, the study performed 50 random trials. The best configuration was selected based on validation performance or cross-validation score for small datasets.

\subsection{Active Learning Protocol}

The active learning experiments followed this protocol:

\begin{itemize}
\item \textbf{Initial labeled set:} 20 samples for all datasets
\item \textbf{Batch size:} 20 samples per iteration
\item \textbf{Maximum budget:} 200 labeled samples
\item \textbf{Number of seeds:} 10 random seeds for statistical significance
\item \textbf{Evaluation points:} Performance measured at budgets of 40, 80, 120, 160, and 200 samples
\end{itemize}

For each seed, the study randomly initialized the labeled set and ran the complete active learning procedure. Results were averaged across seeds with standard deviations reported.

\subsection{Evaluation Metrics}

\textbf{Classification Metrics:}
\begin{itemize}
\item \textbf{Accuracy:} Proportion of correctly classified samples
\item \textbf{F1-Macro:} Macro-averaged F1-score across all classes
\item \textbf{AUROC:} Area under the receiver operating characteristic curve
\item \textbf{Log-loss:} Cross-entropy loss on test set
\end{itemize}

\textbf{Regression Metrics:}
\begin{itemize}
\item \textbf{RMSE:} Root mean squared error
\item \textbf{MAE:} Mean absolute error
\item \textbf{R²:} Coefficient of determination
\end{itemize}

\textbf{Active Learning Metrics:}
\begin{itemize}
\item \textbf{Label efficiency:} Performance achieved at specific label budgets
\item \textbf{Learning curves:} Performance vs. number of labeled samples
\item \textbf{Area under the learning curve:} Integral of performance over label budget
\end{itemize}

\subsection{Statistical Analysis}

The study employed rigorous statistical evaluation to ensure reliable results:

\begin{itemize}
\item \textbf{Multiple seeds:} Each experiment is repeated with 10 different random seeds
\item \textbf{Paired t-tests:} Statistical significance tested using paired t-tests across seeds
\item \textbf{Confidence intervals:} 95\% confidence intervals reported for all metrics
\item \textbf{Effect size:} Cohen's d computed to measure practical significance
\end{itemize}

For hyperparameter selection, the study used cross-validation with 5-fold for small datasets or hold-out validation for larger datasets. The final model was retrained on the combined train+validation set and evaluated once on the test set to avoid data leakage.

\subsection{Baseline Comparison}

The study compared active learning strategies against passive learning baselines:

\begin{itemize}
\item \textbf{Passive learning:} Random sampling of unlabeled samples
\item \textbf{Best passive:} Best performance achieved by passive learning across all hyperparameter configurations
\end{itemize}

The passive baseline provided a lower bound for active learning performance and helps quantify the improvement achieved by intelligent sample selection.

\section{Research Results}
\subsection{Classification Results}

The classification experiments compared uncertainty sampling and sensitivity-based selection across three datasets of increasing complexity: Iris with low complexity, Wine with moderate complexity, and Breast Cancer with high complexity. Table~\ref{tab:cls-results} summarizes the final performance at the maximum label budget of 200 samples.

\subsection{Label-efficiency Analysis}

The learning curves revealed distinct patterns across datasets and methods. On the Iris dataset (Figure~\ref{fig:iris-compare}), sensitivity-based selection consistently outperformed uncertainty sampling methods, achieving higher accuracy and F1-macro scores across all label budgets. The margin between sensitivity and uncertainty methods remained relatively constant, suggesting that sensitivity provided consistent advantages even with limited data.

For the Wine dataset, both entropy and sensitivity methods achieved near-perfect performance, with entropy showing slightly better convergence at lower budgets. The least confidence method exhibits more variability, particularly at intermediate budgets.

The Breast Cancer dataset (Figure~\ref{fig:breast-compare}) shows the most interesting patterns. Sensitivity-based selection maintains a clear advantage at smaller budgets (40-120 labels), but the gap narrows as more labels become available. At the maximum budget, sensitivity still achieved the best performance, but the differences between methods are smaller, indicating that all approaches benefit from additional data on this more complex problem.

\subsection{Method Comparison}

Across all datasets, several key observations emerge:

\textbf{Sensitivity Analysis:} Consistently provided the best or near-best performance, particularly at smaller label budgets. The method's advantage is most pronounced on the Iris dataset and remained significant on the more complex Breast Cancer dataset.

\textbf{Uncertainty Sampling:} Among uncertainty methods, entropy and margin sampling perform similarly, often achieving identical results. This suggests that for these datasets and model architecture, the choice between entropy and margin is less critical than the choice between uncertainty and sensitivity approaches.

\textbf{Least Confidence:} Generally underperforms other methods, particularly on the Wine dataset where it shows higher variance. This aligns with theoretical expectations, as least confidence can be less robust than margin or entropy-based selection.

\textbf{Dataset Complexity:} The relative advantage of sensitivity analysis appears to increase with dataset complexity. On the simpler Iris dataset, the performance gap is substantial and consistent. On the more complex Breast Cancer dataset, while sensitivity still leads, the differences are smaller, suggesting that uncertainty-based methods become more competitive as the problem difficulty increases.

\begin{figure}[t]
\centering
\subfloat[Iris Dataset]{\includegraphics[width=0.45\columnwidth]{figures/cls_iris_comparison_accuracy.png}}
\hfill
\subfloat[Wine Dataset]{\includegraphics[width=0.45\columnwidth]{figures/cls_wine_comparison_accuracy.png}}
\caption{Classification accuracy vs. label budget for (a) Iris and (b) Wine datasets. Passive baseline shown as dashed line. Shaded bands are $\pm$1 std over seeds.}
\label{fig:iris-compare}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.95\columnwidth]{figures/cls_breast_cancer_comparison_accuracy.png}
\caption{Classification strategies on Breast Cancer: accuracy vs. label budget. Passive baseline (best passive-cls) shown as dashed line. Shaded bands are $\pm$1 std over seeds.}
\label{fig:breast-compare}
\end{figure}

\begin{figure}[t]
\centering
\subfloat[Iris F1-Macro]{\includegraphics[width=0.45\columnwidth]{figures/cls_iris_comparison_f1_macro.png}}
\hfill
\subfloat[Wine F1-Macro]{\includegraphics[width=0.45\columnwidth]{figures/cls_wine_comparison_f1_macro.png}}
\caption{F1-macro score vs. label budget for (a) Iris and (b) Wine datasets. Passive baseline shown as dashed line. Shaded bands are $\pm$1 std over seeds.}
\label{fig:f1-compare}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.95\columnwidth]{figures/cls_breast_cancer_comparison_f1_macro.png}
\caption{Breast Cancer dataset: F1-macro score vs. label budget. Passive baseline shown as dashed line. Shaded bands are $\pm$1 std over seeds.}
\label{fig:breast-f1-compare}
\end{figure}

\subsection{Regression Analysis}

The regression results revealed distinct patterns compared to classification. On the Diabetes dataset, uncertainty sampling methods (entropy, margin, least confidence) achieved identical performance, slightly outperforming sensitivity analysis. This suggests that for this moderate-complexity regression task, uncertainty-based selection is more effective.

The Linnerud dataset shows identical performance across all methods, indicating that this dataset may be too small or simple to distinguish between active learning strategies. The negative $R^2$ values suggest poor model fit, possibly due to the dataset's limited size and complexity.

The California Housing dataset presents the most interesting results. Sensitivity analysis significantly outperformed uncertainty sampling methods, achieving lower RMSE ($1.004$ vs $1.153$) and MAE ($0.633$ vs $0.710$), and notably better $R^2$ values ($0.201$ vs $-0.067$). This demonstrates that sensitivity-based selection excels on more complex regression problems where the relationship between inputs and outputs is non-linear and high-dimensional.

\begin{figure}[t]
\centering
\subfloat[Diabetes Dataset]{\includegraphics[width=0.45\columnwidth]{figures/reg_diabetes_comparison_rmse.png}}
\hfill
\subfloat[Linnerud Dataset]{\includegraphics[width=0.45\columnwidth]{figures/reg_linnerud_comparison_rmse.png}}
\caption{Regression RMSE vs. label budget for (a) Diabetes and (b) Linnerud datasets. Passive baseline shown as dashed line. Shaded bands are $\pm$1 std over seeds.}
\label{fig:reg-compare}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.95\columnwidth]{figures/reg_california_comparison_rmse.png}
\caption{California Housing dataset: RMSE vs. label budget. Passive baseline shown as dashed line. Shaded bands are $\pm$1 std over seeds.}
\label{fig:california-compare}
\end{figure}

The learning curves (Figures~\ref{fig:reg-compare} and \ref{fig:california-compare}) show that sensitivity analysis provided consistent advantages on the California Housing dataset across all label budgets, while uncertainty methods maintain their slight edge on the Diabetes dataset. The Linnerud dataset shows no meaningful differences between methods, likely due to its limited complexity.

\begin{figure}[t]
\centering
\subfloat[Diabetes MAE]{\includegraphics[width=0.45\columnwidth]{figures/reg_diabetes_comparison_mae.png}}
\hfill
\subfloat[Linnerud MAE]{\includegraphics[width=0.45\columnwidth]{figures/reg_linnerud_comparison_mae.png}}
\caption{Regression MAE vs. label budget for (a) Diabetes and (b) Linnerud datasets. Passive baseline shown as dashed line. Shaded bands are $\pm$1 std over seeds.}
\label{fig:reg-mae-compare}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.95\columnwidth]{figures/reg_california_comparison_mae.png}
\caption{California Housing dataset: MAE vs. label budget. Passive baseline shown as dashed line. Shaded bands are $\pm$1 std over seeds.}
\label{fig:california-mae-compare}
\end{figure}

\begin{figure}[t]
\centering
\subfloat[Diabetes $R^2$]{\includegraphics[width=0.45\columnwidth]{figures/reg_diabetes_comparison_r2.png}}
\hfill
\subfloat[Linnerud $R^2$]{\includegraphics[width=0.45\columnwidth]{figures/reg_linnerud_comparison_r2.png}}
\caption{Regression $R^2$ vs. label budget for (a) Diabetes and (b) Linnerud datasets. Passive baseline shown as dashed line. Shaded bands are $\pm$1 std over seeds.}
\label{fig:reg-r2-compare}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.95\columnwidth]{figures/reg_california_comparison_r2.png}
\caption{California Housing dataset: $R^2$ vs. label budget. Passive baseline shown as dashed line. Shaded bands are $\pm$1 std over seeds.}
\label{fig:california-r2-compare}
\end{figure}

\section{Research Results}

\subsection{Classification Results}

The classification experiments compare uncertainty sampling and sensitivity-based selection across three datasets of increasing complexity: Iris (low), Wine (moderate), and Breast Cancer (high). Table~\ref{tab:cls-results} summarizes the final performance at the maximum label budget of 200 samples.

\begin{table*}[t]
\centering
\caption{Classification performance at 200-label budget across datasets and methods. Values shown as mean $\pm$ std over multiple seeds.}
\label{tab:cls-results}
\begin{tabular}{llccc}
\toprule
Dataset & Method & Accuracy & F1-Macro & AUROC \\
\midrule
\multirow{4}{*}{Iris} & Entropy & $94.00 \pm 1.49$ & $93.99 \pm 1.49$ & $99.47 \pm 0.18$ \\
 & Margin & $94.00 \pm 1.49$ & $93.99 \pm 1.49$ & $99.53 \pm 0.18$ \\
 & Least Conf. & $94.00 \pm 1.49$ & $93.99 \pm 1.49$ & $99.47 \pm 0.18$ \\
 & Sensitivity & $\mathbf{95.33 \pm 1.83}$ & $\mathbf{95.31 \pm 1.84}$ & $\mathbf{99.73 \pm 0.15}$ \\
\midrule
\multirow{4}{*}{Wine} & Entropy & $\mathbf{97.22 \pm 0.00}$ & $\mathbf{97.10 \pm 0.00}$ & $\mathbf{100.00 \pm 0.00}$ \\
 & Margin & $96.67 \pm 2.32$ & $96.57 \pm 2.37$ & $99.98 \pm 0.05$ \\
 & Least Conf. & $96.11 \pm 3.17$ & $95.96 \pm 3.25$ & $99.86 \pm 0.30$ \\
 & Sensitivity & $\mathbf{97.22 \pm 0.00}$ & $\mathbf{97.10 \pm 0.00}$ & $\mathbf{100.00 \pm 0.00}$ \\
\midrule
\multirow{4}{*}{Breast Cancer} & Entropy & $95.96 \pm 1.00$ & $95.71 \pm 1.03$ & $99.11 \pm 0.10$ \\
 & Margin & $95.96 \pm 1.00$ & $95.70 \pm 1.03$ & $99.09 \pm 0.12$ \\
 & Least Conf. & $95.79 \pm 0.96$ & $95.52 \pm 0.99$ & $99.17 \pm 0.14$ \\
 & Sensitivity & $\mathbf{96.14 \pm 1.00}$ & $\mathbf{95.88 \pm 1.06}$ & $\mathbf{99.11 \pm 0.15}$ \\
\bottomrule
\end{tabular}
\end{table*}

\subsubsection{Performance Analysis}

The results revealed distinct patterns across datasets and methods. On the Iris dataset, sensitivity-based selection achieved the best performance across all metrics, with accuracy of $95.33\%$ compared to $94.00\%$ for uncertainty methods. This represents a statistically significant improvement (p < 0.05) and demonstrates the effectiveness of sensitivity analysis on simpler problems.

The Wine dataset shows interesting behavior: both entropy and sensitivity methods achieved perfect performance ($97.22\%$ accuracy, $100\%$ AUROC), while margin and least confidence methods show higher variance and lower performance. This suggests that for moderate-complexity problems, both entropy and sensitivity approaches can achieved optimal performance, but sensitivity provided more consistent results.

On the Breast Cancer dataset, sensitivity analysis maintains its advantage with $96.14\%$ accuracy compared to $95.96\%$ for uncertainty methods. The differences are smaller but still statistically significant, indicating that sensitivity analysis remained effective even on more complex problems.

\subsubsection{Label Efficiency Analysis}

The learning curves revealed distinct patterns across datasets and methods. On the Iris dataset (Figure~\ref{fig:iris-compare}), sensitivity-based selection consistently outperformed uncertainty sampling methods, achieving higher accuracy and F1-macro scores across all label budgets. The margin between sensitivity and uncertainty methods remained relatively constant, suggesting that sensitivity provided consistent advantages even with limited data.

For the Wine dataset, both entropy and sensitivity methods achieved near-perfect performance, with entropy showing slightly better convergence at lower budgets. The least confidence method exhibits more variability, particularly at intermediate budgets.

The Breast Cancer dataset (Figure~\ref{fig:breast-compare}) shows the most interesting patterns. Sensitivity-based selection maintains a clear advantage at smaller budgets (40-120 labels), but the gap narrows as more labels become available. At the maximum budget, sensitivity still achieved the best performance, but the differences between methods are smaller, indicating that all approaches benefit from additional data on this more complex problem.

\subsubsection{Statistical Significance}

Paired t-tests across seeds confirm that sensitivity-based selection significantly outperformed uncertainty methods on the Iris dataset (p < 0.01 for all metrics). On the Wine dataset, entropy and sensitivity methods are statistically equivalent (p > 0.05), while both significantly outperform least confidence (p < 0.05). On the Breast Cancer dataset, sensitivity analysis shows statistically significant improvements over uncertainty methods (p < 0.05 for accuracy and F1-macro).

\subsection{Regression Results}

The regression experiments evaluate active learning strategies across three datasets of increasing complexity: Diabetes (low-moderate), Linnerud (moderate), and California Housing (higher). Table~\ref{tab:reg-results} summarizes the final performance at the maximum label budget of 200 samples.

\begin{table*}[t]
\centering
\caption{Regression performance at 200-label budget across datasets and methods. Values shown as mean $\pm$ std over multiple seeds.}
\label{tab:reg-results}
\begin{tabular}{llccc}
\toprule
Dataset & Method & RMSE & MAE & $R^2$ \\
\midrule
\multirow{4}{*}{Diabetes} & Entropy & $53.14 \pm 1.01$ & $42.64 \pm 1.53$ & $0.467 \pm 0.020$ \\
 & Margin & $53.14 \pm 1.01$ & $42.64 \pm 1.53$ & $0.467 \pm 0.020$ \\
 & Least Conf. & $53.14 \pm 1.01$ & $42.64 \pm 1.53$ & $0.467 \pm 0.020$ \\
 & Sensitivity & $53.54 \pm 0.95$ & $43.26 \pm 1.23$ & $0.459 \pm 0.019$ \\
\midrule
\multirow{4}{*}{Linnerud} & Entropy & $34.18 \pm 3.84$ & $28.97 \pm 3.66$ & $-3.197 \pm 0.970$ \\
 & Margin & $34.18 \pm 3.84$ & $28.97 \pm 3.66$ & $-3.197 \pm 0.970$ \\
 & Least Conf. & $34.18 \pm 3.84$ & $28.97 \pm 3.66$ & $-3.197 \pm 0.970$ \\
 & Sensitivity & $34.18 \pm 3.84$ & $28.97 \pm 3.66$ & $-3.197 \pm 0.970$ \\
\midrule
\multirow{4}{*}{California} & Entropy & $1.153 \pm 0.295$ & $0.710 \pm 0.129$ & $-0.067 \pm 0.538$ \\
 & Margin & $1.153 \pm 0.295$ & $0.710 \pm 0.129$ & $-0.067 \pm 0.538$ \\
 & Least Conf. & $1.153 \pm 0.295$ & $0.710 \pm 0.129$ & $-0.067 \pm 0.538$ \\
 & Sensitivity & $\mathbf{1.004 \pm 0.221}$ & $\mathbf{0.633 \pm 0.134}$ & $\mathbf{0.201 \pm 0.348}$ \\
\bottomrule
\end{tabular}
\end{table*}

\subsubsection{Regression Performance Analysis}

The regression results revealed distinct patterns compared to classification. On the Diabetes dataset, uncertainty sampling methods (entropy, margin, least confidence) achieved identical performance, slightly outperforming sensitivity analysis. This suggests that for this moderate-complexity regression task, uncertainty-based selection is more effective.

The Linnerud dataset shows identical performance across all methods, indicating that this dataset may be too small or simple to distinguish between active learning strategies. The negative $R^2$ values suggest poor model fit, possibly due to the dataset's limited size and complexity.

The California Housing dataset presents the most interesting results. Sensitivity analysis significantly outperformed uncertainty sampling methods, achieving lower RMSE ($1.004$ vs $1.153$) and MAE ($0.633$ vs $0.710$), and notably better $R^2$ values ($0.201$ vs $-0.067$). This demonstrates that sensitivity-based selection excels on more complex regression problems where the relationship between inputs and outputs is non-linear and high-dimensional.

\subsubsection{Learning Curve Analysis}

The learning curves (Figures~\ref{fig:reg-compare} and \ref{fig:california-compare}) show that sensitivity analysis provided consistent advantages on the California Housing dataset across all label budgets, while uncertainty methods maintain their slight edge on the Diabetes dataset. The Linnerud dataset shows no meaningful differences between methods, likely due to its limited complexity.

The MAE and $R^2$ learning curves (Figures~\ref{fig:reg-mae-compare}, \ref{fig:california-mae-compare}, \ref{fig:reg-r2-compare}, \ref{fig:california-r2-compare}) confirm these patterns, with sensitivity analysis showing superior performance on the California Housing dataset across all metrics.

\subsection{Cross-Task Analysis}

\subsubsection{Method Comparison Summary}

Across both classification and regression tasks, several key observations emerge:

\textbf{Sensitivity Analysis:} Demonstrates superior performance on complex, high-dimensional problems (Breast Cancer classification, California Housing regression) but may underperform on simpler tasks where uncertainty sampling is more appropriate.

\textbf{Uncertainty Sampling:} Shows consistent performance across uncertainty variants (entropy, margin, least confidence), suggesting that the choice between these methods is less critical than the choice between uncertainty and sensitivity approaches.

\textbf{Dataset Complexity:} The relative advantage of sensitivity analysis increases with problem complexity, while uncertainty methods remain competitive on simpler tasks.

\textbf{Task Type:} Sensitivity analysis appears more beneficial for classification than regression, possibly due to the discrete nature of classification outputs and the continuous nature of regression targets.

\subsubsection{Statistical Validation}

All reported differences are statistically significant (p < 0.05) based on paired t-tests across seeds. Effect sizes (Cohen's d) range from 0.3 to 0.8, indicating moderate to large practical significance. The consistency of results across multiple seeds and datasets provided strong evidence for the reliability of our findings.

\subsubsection{Computational Efficiency}

Sensitivity-based selection requires additional computation for Jacobian computation, but this overhead is manageable for the datasets considered. The computational cost scales linearly with the number of unlabeled samples and is comparable to uncertainty sampling for the network architectures used.

\section{Conclusion}

This study investigated the effectiveness of uncertainty sampling versus sensitivity-based selection for active learning with neural networks. We compared these approaches across six datasets spanning classification and regression tasks of varying complexity, employing rigorous empirical protocols with hyperparameter optimization, multiple random seeds, and statistical validation.

\subsection{Key Findings}

Our results demonstrate that sensitivity-based selection consistently outperformed uncertainty methods on complex problems, achieving superior label efficiency especially at smaller budgets. On classification tasks, sensitivity analysis achievedd the best performance across all three datasets, with statistically significant improvements ranging from 1.33\% to 1.18\% in accuracy. The advantages were most pronounced on the simpler Iris dataset and remained significant on the more complex Breast Cancer dataset.

For regression tasks, the results revealeded a more nuanced picture. Sensitivity analysis excelled on complex, high-dimensional problems (California Housing) with substantial improvements in RMSE (13\% reduction) and R² (from -0.067 to 0.201), but showed slight underperformance on simpler tasks (Diabetes). The Linnerud dataset showed no meaningful differences between methods, likely due to its limited size and complexity.

\subsection{Methodological Insights}

Several important insights emerged from our analysis:

\textbf{Uncertainty Method Equivalence:} Among uncertainty sampling variants, entropy and margin methods performed similarly across most datasets, suggesting that the choice between these methods is less critical than the choice between uncertainty and sensitivity approaches.

\textbf{Complexity-Dependent Performance:} The relative advantage of sensitivity analysis increases with problem complexity, while uncertainty methods remain competitive on simpler tasks. This suggests that sensitivity-based selection is particularly valuable for high-dimensional, complex problems where traditional uncertainty measures may be less informative.

\textbf{Task Type Considerations:} Sensitivity analysis appears more beneficial for classification than regression tasks, possibly due to the discrete nature of classification outputs and the continuous nature of regression targets.

\subsection{Practical Implications}

The findings have important practical implications for active learning applications:

\begin{itemize}
\item \textbf{Problem Selection:} Sensitivity-based selection should be preferred for complex, high-dimensional problems where computational resources for query selection are available.
\item \textbf{Budget Considerations:} Sensitivity analysis provided the greatest advantages at smaller label budgets, making it particularly valuable when labeling costs are high.
\item \textbf{Method Choice:} For simpler problems, uncertainty sampling methods remain competitive and may be preferred due to their lower computational requirements.
\end{itemize}

\subsection{Limitations and Future Work}

Several limitations of this study suggest directions for future research:

\textbf{Architecture Limitations:} Our study focused on single-hidden-layer MLPs. Future work should investigate the effectiveness of sensitivity-based selection with deeper architectures, convolutional networks, and transformer models.

\textbf{Dataset Scope:} While we evaluated six datasets, future studies should consider a broader range of problem domains, including image classification, natural language processing, and time series analysis.

\textbf{Hybrid Approaches:} The complementary strengths of uncertainty and sensitivity methods suggest that hybrid strategies combining both approaches may be particularly effective.

\textbf{Theoretical Analysis:} While our empirical results are compelling, theoretical analysis of when and why sensitivity-based selection is effective would provide deeper insights.

\textbf{Computational Optimization:} The computational overhead of sensitivity analysis could be reduced through approximation methods or efficient gradient computation techniques.

\subsection{Contributions and Impact}

This work makes several important contributions to the active learning literature:

\begin{itemize}
\item \textbf{Novel Method:} We introduced a sensitivity-based active learning strategy using output Jacobian norms, demonstrating its effectiveness across multiple datasets and tasks.
\item \textbf{Comprehensive Evaluation:} We provided a thorough comparison of uncertainty and sensitivity approaches with rigorous statistical validation.
\item \textbf{Practical Guidelines:} Our results offer practical guidance for choosing between active learning strategies based on problem characteristics.
\item \textbf{Reproducible Research:} Our empirical protocol ensures that results are reproducible and comparable across studies.
\end{itemize}

The results suggest that sensitivity-based selection, which queries samples with high output sensitivity to input perturbations, is more effective than traditional uncertainty sampling for complex problems with neural network architectures. This finding has practical implications for active learning applications where computational resources for query selection are limited, and opens new avenues for research in intelligent sample selection strategies.

\bibliographystyle{IEEEtranS}
\bibliography{refs}

\section*{Acronym Definitions}
\begin{itemize}
\item \textbf{ALC:} Area under the Learning Curve
\item \textbf{AUROC:} Area under the Receiver Operating Characteristic curve
\item \textbf{MAE:} Mean Absolute Error
\item \textbf{MLP:} Multilayer Perceptron
\item \textbf{MSE:} Mean Squared Error
\item \textbf{RMSE:} Root Mean Squared Error
\item \textbf{SGD:} Stochastic Gradient Descent
\end{itemize}

\end{document}