{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare Active Learning Strategies - Classification (with CV + Multiple Trials)\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "SAVE_DIR = os.path.join('..', 'report', 'figures')\n",
        "DATA_DIR = os.path.join('..', 'data')\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# Load results (these should be generated by the updated notebooks)\n",
        "try:\n",
        "    with open(os.path.join(DATA_DIR, 'cls_uncertainty_results.json'), 'r') as f:\n",
        "        unc = json.load(f)\n",
        "    with open(os.path.join(DATA_DIR, 'cls_sensitivity_results.json'), 'r') as f:\n",
        "        sen = json.load(f)\n",
        "    with open(os.path.join(DATA_DIR, 'passive_cls_best.json'), 'r') as f:\n",
        "        pas = json.load(f)\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Results file not found: {e}\")\n",
        "    print(\"Please run the updated exploration notebooks first to generate results with CV + multiple trials\")\n",
        "    exit()\n",
        "\n",
        "DATASETS = ['iris', 'wine', 'breast_cancer']\n",
        "METRICS = ['accuracy', 'f1_macro']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Comparing strategies for iris ===\n",
            "Saved iris accuracy comparison\n",
            "Saved iris f1_macro comparison\n",
            "\n",
            "=== Comparing strategies for wine ===\n",
            "Saved wine accuracy comparison\n",
            "Saved wine f1_macro comparison\n",
            "\n",
            "=== Comparing strategies for breast_cancer ===\n",
            "Saved breast_cancer accuracy comparison\n",
            "Saved breast_cancer f1_macro comparison\n"
          ]
        }
      ],
      "source": [
        "# Compare all datasets\n",
        "for dataset in DATASETS:\n",
        "    print(f\"\\n=== Comparing strategies for {dataset} ===\")\n",
        "    \n",
        "    # Get budgets and methods\n",
        "    budgets = sorted([int(b) for b in next(iter(unc[dataset].values())).keys()])\n",
        "    methods = list(unc[dataset].keys())\n",
        "    \n",
        "    for metric in METRICS:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        \n",
        "        # Plot uncertainty methods\n",
        "        for m in methods:\n",
        "            means = [unc[dataset][m][str(b)][f'{metric}_mean'] for b in budgets]\n",
        "            stds = [unc[dataset][m][str(b)][f'{metric}_std'] for b in budgets]\n",
        "            plt.plot(budgets, means, marker='o', label=f'uncertainty_{m}', linewidth=2)\n",
        "            plt.fill_between(budgets, np.array(means)-np.array(stds), np.array(means)+np.array(stds), alpha=0.2)\n",
        "        \n",
        "        # Plot sensitivity method\n",
        "        if dataset in sen:\n",
        "            means = [sen[dataset][str(b)][f'{metric}_mean'] for b in budgets]\n",
        "            stds = [sen[dataset][str(b)][f'{metric}_std'] for b in budgets]\n",
        "            plt.plot(budgets, means, marker='s', label='sensitivity', linewidth=2)\n",
        "            plt.fill_between(budgets, np.array(means)-np.array(stds), np.array(means)+np.array(stds), alpha=0.2)\n",
        "        \n",
        "        # Plot passive baseline as horizontal line\n",
        "        baseline = pas[dataset]['best_metric']\n",
        "        plt.axhline(baseline, color='k', linestyle='--', label='passive_best', linewidth=2)\n",
        "        \n",
        "        plt.xlabel('Labeled budget (max_labels)')\n",
        "        plt.ylabel(metric)\n",
        "        plt.title(f'{dataset}: Strategies Comparison ({metric}) - CV + Multiple Trials')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(SAVE_DIR, f'cls_{dataset}_comparison_{metric}.png'), dpi=200, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        \n",
        "        print(f\"Saved {dataset} {metric} comparison\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Summary Table ===\n",
            "          dataset            method  budget  accuracy_mean  accuracy_std  \\\n",
            "0            iris           entropy     200         0.9400        0.0149   \n",
            "1            iris            margin     200         0.9400        0.0149   \n",
            "2            iris  least_confidence     200         0.9400        0.0149   \n",
            "3            iris       sensitivity     200         0.9533        0.0183   \n",
            "4            wine           entropy     200         0.9722        0.0000   \n",
            "5            wine            margin     200         0.9667        0.0232   \n",
            "6            wine  least_confidence     200         0.9611        0.0317   \n",
            "7            wine       sensitivity     200         0.9722        0.0000   \n",
            "8   breast_cancer           entropy     200         0.9596        0.0100   \n",
            "9   breast_cancer            margin     200         0.9596        0.0100   \n",
            "10  breast_cancer  least_confidence     200         0.9579        0.0096   \n",
            "11  breast_cancer       sensitivity     200         0.9614        0.0100   \n",
            "\n",
            "    f1_mean  f1_std  \n",
            "0    0.9400  0.0149  \n",
            "1    0.9400  0.0149  \n",
            "2    0.9400  0.0149  \n",
            "3    0.9531  0.0184  \n",
            "4    0.9710  0.0000  \n",
            "5    0.9657  0.0237  \n",
            "6    0.9596  0.0325  \n",
            "7    0.9710  0.0000  \n",
            "8    0.9571  0.0103  \n",
            "9    0.9570  0.0103  \n",
            "10   0.9552  0.0099  \n",
            "11   0.9588  0.0106  \n",
            "\n",
            "Saved comparison summary to ../report/figures\n",
            "All comparison figures and summary saved!\n"
          ]
        }
      ],
      "source": [
        "# Create summary table\n",
        "print(\"\\n=== Summary Table ===\")\n",
        "summary_data = []\n",
        "\n",
        "for dataset in DATASETS:\n",
        "    # Get budgets and methods\n",
        "    budgets = sorted([int(b) for b in next(iter(unc[dataset].values())).keys()])\n",
        "    methods = list(unc[dataset].keys())\n",
        "    \n",
        "    for method in methods + ['sensitivity']:\n",
        "        if method == 'sensitivity' and dataset not in sen:\n",
        "            continue\n",
        "            \n",
        "        if method == 'sensitivity':\n",
        "            data = sen[dataset]\n",
        "        else:\n",
        "            data = unc[dataset][method]\n",
        "        \n",
        "        # Get performance at highest budget\n",
        "        max_budget = max(budgets)\n",
        "        acc_mean = data[str(max_budget)]['accuracy_mean']\n",
        "        acc_std = data[str(max_budget)]['accuracy_std']\n",
        "        f1_mean = data[str(max_budget)]['f1_macro_mean']\n",
        "        f1_std = data[str(max_budget)]['f1_macro_std']\n",
        "        \n",
        "        summary_data.append({\n",
        "            'dataset': dataset,\n",
        "            'method': method,\n",
        "            'budget': max_budget,\n",
        "            'accuracy_mean': acc_mean,\n",
        "            'accuracy_std': acc_std,\n",
        "            'f1_mean': f1_mean,\n",
        "            'f1_std': f1_std\n",
        "        })\n",
        "\n",
        "# Convert to DataFrame for nice display\n",
        "df = pd.DataFrame(summary_data)\n",
        "print(df.round(4))\n",
        "\n",
        "# Save summary\n",
        "df.to_csv(os.path.join(SAVE_DIR, 'cls_comparison_summary.csv'), index=False)\n",
        "print(f'\\nSaved comparison summary to {SAVE_DIR}')\n",
        "print('All comparison figures and summary saved!')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Machine-Learning-441-fvr4FmDE",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
