{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare Active Learning Strategies - Classification (with CV + Multiple Trials)\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "SAVE_DIR = os.path.join('..', 'report', 'figures')\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# Load results (these should be generated by the updated notebooks)\n",
        "try:\n",
        "    with open(os.path.join(SAVE_DIR, 'cls_uncertainty_results.json'), 'r') as f:\n",
        "        unc = json.load(f)\n",
        "    with open(os.path.join(SAVE_DIR, 'cls_sensitivity_results.json'), 'r') as f:\n",
        "        sen = json.load(f)\n",
        "    with open(os.path.join(SAVE_DIR, 'passive_cls_best.json'), 'r') as f:\n",
        "        pas = json.load(f)\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Results file not found: {e}\")\n",
        "    print(\"Please run the updated exploration notebooks first to generate results with CV + multiple trials\")\n",
        "    exit()\n",
        "\n",
        "DATASETS = ['iris', 'wine', 'breast_cancer']\n",
        "METRICS = ['accuracy', 'f1_macro']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare all datasets\n",
        "for dataset in DATASETS:\n",
        "    print(f\"\\n=== Comparing strategies for {dataset} ===\")\n",
        "    \n",
        "    # Get budgets and methods\n",
        "    budgets = sorted([int(b) for b in next(iter(unc[dataset].values())).keys()])\n",
        "    methods = list(unc[dataset].keys())\n",
        "    \n",
        "    for metric in METRICS:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        \n",
        "        # Plot uncertainty methods\n",
        "        for m in methods:\n",
        "            means = [unc[dataset][m][str(b)][f'{metric}_mean'] for b in budgets]\n",
        "            stds = [unc[dataset][m][str(b)][f'{metric}_std'] for b in budgets]\n",
        "            plt.plot(budgets, means, marker='o', label=f'uncertainty_{m}', linewidth=2)\n",
        "            plt.fill_between(budgets, np.array(means)-np.array(stds), np.array(means)+np.array(stds), alpha=0.2)\n",
        "        \n",
        "        # Plot sensitivity method\n",
        "        if dataset in sen:\n",
        "            means = [sen[dataset][str(b)][f'{metric}_mean'] for b in budgets]\n",
        "            stds = [sen[dataset][str(b)][f'{metric}_std'] for b in budgets]\n",
        "            plt.plot(budgets, means, marker='s', label='sensitivity', linewidth=2)\n",
        "            plt.fill_between(budgets, np.array(means)-np.array(stds), np.array(means)+np.array(stds), alpha=0.2)\n",
        "        \n",
        "        # Plot passive baseline as horizontal line\n",
        "        baseline = pas[dataset]['best_metric']\n",
        "        plt.axhline(baseline, color='k', linestyle='--', label='passive_best', linewidth=2)\n",
        "        \n",
        "        plt.xlabel('Labeled budget (max_labels)')\n",
        "        plt.ylabel(metric)\n",
        "        plt.title(f'{dataset}: Strategies Comparison ({metric}) - CV + Multiple Trials')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(SAVE_DIR, f'cls_{dataset}_comparison_{metric}.png'), dpi=200, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        \n",
        "        print(f\"Saved {dataset} {metric} comparison\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create summary table\n",
        "print(\"\\n=== Summary Table ===\")\n",
        "summary_data = []\n",
        "\n",
        "for dataset in DATASETS:\n",
        "    # Get budgets and methods\n",
        "    budgets = sorted([int(b) for b in next(iter(unc[dataset].values())).keys()])\n",
        "    methods = list(unc[dataset].keys())\n",
        "    \n",
        "    for method in methods + ['sensitivity']:\n",
        "        if method == 'sensitivity' and dataset not in sen:\n",
        "            continue\n",
        "            \n",
        "        if method == 'sensitivity':\n",
        "            data = sen[dataset]\n",
        "        else:\n",
        "            data = unc[dataset][method]\n",
        "        \n",
        "        # Get performance at highest budget\n",
        "        max_budget = max(budgets)\n",
        "        acc_mean = data[str(max_budget)]['accuracy_mean']\n",
        "        acc_std = data[str(max_budget)]['accuracy_std']\n",
        "        f1_mean = data[str(max_budget)]['f1_macro_mean']\n",
        "        f1_std = data[str(max_budget)]['f1_macro_std']\n",
        "        \n",
        "        summary_data.append({\n",
        "            'dataset': dataset,\n",
        "            'method': method,\n",
        "            'budget': max_budget,\n",
        "            'accuracy_mean': acc_mean,\n",
        "            'accuracy_std': acc_std,\n",
        "            'f1_mean': f1_mean,\n",
        "            'f1_std': f1_std\n",
        "        })\n",
        "\n",
        "# Convert to DataFrame for nice display\n",
        "df = pd.DataFrame(summary_data)\n",
        "print(df.round(4))\n",
        "\n",
        "# Save summary\n",
        "df.to_csv(os.path.join(SAVE_DIR, 'cls_comparison_summary.csv'), index=False)\n",
        "print(f'\\nSaved comparison summary to {SAVE_DIR}')\n",
        "print('All comparison figures and summary saved!')\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
