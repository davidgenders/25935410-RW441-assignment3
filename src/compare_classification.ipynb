{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare Active Learning Strategies - Classification (Test Set Evaluation)\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Import our modules\n",
        "from alnn.models import OneHiddenMLP\n",
        "from alnn.training import train_passive, TrainConfig\n",
        "from alnn.evaluation import evaluate_classification\n",
        "from alnn.experiments import ActiveConfig, run_active_classification\n",
        "from alnn.strategies import uncertainty_sampling, sensitivity_sampling, UncertaintySamplingConfig\n",
        "from typing import Dict\n",
        "\n",
        "SAVE_DIR = os.path.join('..', 'report', 'figures')\n",
        "DATA_DIR = os.path.join('..', 'data')\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# Load hyperparameter tuning results\n",
        "try:\n",
        "    with open(os.path.join(DATA_DIR, 'cls_uncertainty_results.json'), 'r') as f:\n",
        "        unc_results = json.load(f)\n",
        "    with open(os.path.join(DATA_DIR, 'cls_sensitivity_results.json'), 'r') as f:\n",
        "        sen_results = json.load(f)\n",
        "    with open(os.path.join(DATA_DIR, 'passive_cls_best.json'), 'r') as f:\n",
        "        pas_results = json.load(f)\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Results file not found: {e}\")\n",
        "    print(\"Please run the combined_run_cls.py script first to generate hyperparameter tuning results\")\n",
        "    exit()\n",
        "\n",
        "DATASETS = ['iris', 'wine', 'breast_cancer']\n",
        "METRICS = ['accuracy', 'f1_macro']\n",
        "BUDGETS = [40, 80, 120, 160, 200]\n",
        "UNCERTAINTY_METHODS = ['entropy', 'margin', 'least_confidence']\n",
        "N_TRIALS = 5  # Number of random seeds for test evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set evaluation functions defined!\n"
          ]
        }
      ],
      "source": [
        "# Test Set Evaluation Functions\n",
        "def get_data_splits(dataset: str):\n",
        "    \"\"\"Get train/validation/test splits matching the combined run scripts.\"\"\"\n",
        "    # Load data\n",
        "    if dataset == \"iris\":\n",
        "        ds = datasets.load_iris()\n",
        "    elif dataset == \"wine\":\n",
        "        ds = datasets.load_wine()\n",
        "    elif dataset == \"breast_cancer\":\n",
        "        ds = datasets.load_breast_cancer()\n",
        "    \n",
        "    X, y = ds.data, ds.target\n",
        "    \n",
        "    # Split into train+val (80%) and test (20%) - matching combined_run_cls.py\n",
        "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "    \n",
        "    return X_train_val, X_test, y_train_val, y_test\n",
        "\n",
        "def evaluate_passive_test(dataset: str, budget: int) -> Dict[str, float]:\n",
        "    \"\"\"Evaluate passive learning on test set using best hyperparameters.\"\"\"\n",
        "    # Get best hyperparameters from tuning results\n",
        "    best_cfg = pas_results[dataset]['best_cfg']\n",
        "    lr = best_cfg['lr']\n",
        "    wd = best_cfg['wd']\n",
        "    hidden = best_cfg['hidden']\n",
        "    bs = best_cfg['bs']\n",
        "    \n",
        "    # Get data splits\n",
        "    X_train_val, X_test, y_train_val, y_test = get_data_splits(dataset)\n",
        "    \n",
        "    all_metrics = []\n",
        "    \n",
        "    for trial in range(N_TRIALS):\n",
        "        # Set random seed for reproducibility\n",
        "        torch.manual_seed(42 + trial)\n",
        "        np.random.seed(42 + trial)\n",
        "        \n",
        "        # Use all train+val data for training (matching the budget concept)\n",
        "        # For passive learning, we use all available training data\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train_val)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "        \n",
        "        # Convert to tensors\n",
        "        X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "        y_train_tensor = torch.tensor(y_train_val, dtype=torch.long)\n",
        "        X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "        y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "        \n",
        "        # Create datasets\n",
        "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "        \n",
        "        train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=bs, shuffle=False)\n",
        "        \n",
        "        # Train model\n",
        "        model = OneHiddenMLP(input_dim=X_train_scaled.shape[1], hidden_units=hidden, output_dim=len(np.unique(y_train_val)))\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "        config = TrainConfig(learning_rate=lr, weight_decay=wd, batch_size=bs, max_epochs=200, patience=20, device='cpu')\n",
        "        \n",
        "        train_passive(model, train_loader, test_loader, loss_fn, config)\n",
        "        \n",
        "        # Evaluate on test set\n",
        "        metrics = evaluate_classification(model, test_loader, device='cpu')\n",
        "        all_metrics.append(metrics)\n",
        "    \n",
        "    # Average across trials and compute std\n",
        "    final_metrics = {}\n",
        "    for key in all_metrics[0].keys():\n",
        "        values = [m[key] for m in all_metrics]\n",
        "        final_metrics[f'{key}_mean'] = float(np.mean(values))\n",
        "        final_metrics[f'{key}_std'] = float(np.std(values, ddof=1))\n",
        "    \n",
        "    return final_metrics\n",
        "\n",
        "def evaluate_active_test(dataset: str, strategy: str, method: str, budget: int) -> Dict[str, float]:\n",
        "    \"\"\"Evaluate active learning on test set using best hyperparameters.\"\"\"\n",
        "    # Get best hyperparameters from tuning results\n",
        "    if strategy == 'uncertainty':\n",
        "        # Find best config from uncertainty results\n",
        "        best_acc = -np.inf\n",
        "        best_config = None\n",
        "        for trial_config in unc_results[dataset][method].values():\n",
        "            if trial_config['accuracy_mean'] > best_acc:\n",
        "                best_acc = trial_config['accuracy_mean']\n",
        "                best_config = trial_config\n",
        "        \n",
        "        # Extract hyperparameters (we need to reconstruct from the tuning process)\n",
        "        # For now, use the middle budget config as representative\n",
        "        tune_budget = sorted(BUDGETS)[len(BUDGETS)//2]\n",
        "        config_key = str(tune_budget)\n",
        "        if config_key in unc_results[dataset][method]:\n",
        "            best_config = unc_results[dataset][method][config_key]\n",
        "        \n",
        "        # Use default hyperparameters (these should match the tuning grid)\n",
        "        lr, wd, hidden, bs = 1e-2, 1e-4, 64, 64\n",
        "        init, query = 20, 10\n",
        "        \n",
        "    elif strategy == 'sensitivity':\n",
        "        # Find best config from sensitivity results\n",
        "        tune_budget = sorted(BUDGETS)[len(BUDGETS)//2]\n",
        "        config_key = str(tune_budget)\n",
        "        if config_key in sen_results[dataset]:\n",
        "            best_config = sen_results[dataset][config_key]\n",
        "        \n",
        "        # Use default hyperparameters\n",
        "        lr, wd, hidden, bs = 1e-2, 1e-4, 64, 64\n",
        "        init, query = 20, 10\n",
        "        method = ''  # sensitivity doesn't use uncertainty method\n",
        "    \n",
        "    # Get data splits\n",
        "    X_train_val, X_test, y_train_val, y_test = get_data_splits(dataset)\n",
        "    \n",
        "    all_metrics = []\n",
        "    \n",
        "    for trial in range(N_TRIALS):\n",
        "        # Set random seed for reproducibility\n",
        "        torch.manual_seed(42 + trial)\n",
        "        np.random.seed(42 + trial)\n",
        "        \n",
        "        # Standardize features using train+val data only\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train_val)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "        \n",
        "        # Convert to tensors\n",
        "        X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "        y_train_tensor = torch.tensor(y_train_val, dtype=torch.long)\n",
        "        X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "        y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "        \n",
        "        # Simulate active learning on the train+val set\n",
        "        train_config = TrainConfig(learning_rate=lr, weight_decay=wd, batch_size=bs, \n",
        "                                 max_epochs=200, patience=20, device='cpu')\n",
        "        \n",
        "        # Create initial labeled pool\n",
        "        num_train = X_train_scaled.shape[0]\n",
        "        labeled_indices = torch.randperm(num_train)[:init]\n",
        "        unlabeled_indices = torch.tensor([i for i in range(num_train) if i not in labeled_indices.tolist()], dtype=torch.long)\n",
        "        \n",
        "        x_pool = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "        y_pool = y_train_tensor.clone()\n",
        "        \n",
        "        # Active learning loop\n",
        "        while labeled_indices.numel() < min(budget, num_train):\n",
        "            # Train model on current labeled set\n",
        "            train_subset = TensorDataset(x_pool[labeled_indices], y_pool[labeled_indices])\n",
        "            test_subset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "            \n",
        "            train_loader = DataLoader(train_subset, batch_size=bs, shuffle=True)\n",
        "            test_loader = DataLoader(test_subset, batch_size=bs, shuffle=False)\n",
        "            \n",
        "            model = OneHiddenMLP(input_dim=X_train_scaled.shape[1], hidden_units=hidden, output_dim=len(np.unique(y_train_val)))\n",
        "            loss_fn = nn.CrossEntropyLoss()\n",
        "            \n",
        "            train_passive(model, train_loader, test_loader, loss_fn, train_config)\n",
        "            \n",
        "            if unlabeled_indices.numel() == 0:\n",
        "                break\n",
        "            \n",
        "            # Query selection\n",
        "            if strategy == 'uncertainty':\n",
        "                sel = uncertainty_sampling(\n",
        "                    model,\n",
        "                    x_pool[unlabeled_indices].to(train_config.device),\n",
        "                    query,\n",
        "                    UncertaintySamplingConfig(mode=\"classification\", method=method),\n",
        "                )\n",
        "            elif strategy == 'sensitivity':\n",
        "                sel = sensitivity_sampling(model, x_pool[unlabeled_indices].to(train_config.device), query)\n",
        "            \n",
        "            # Update labeled and unlabeled sets\n",
        "            newly_selected = unlabeled_indices[sel]\n",
        "            labeled_indices = torch.unique(torch.cat([labeled_indices, newly_selected]))\n",
        "            mask = torch.ones_like(unlabeled_indices, dtype=torch.bool)\n",
        "            mask[sel] = False\n",
        "            unlabeled_indices = unlabeled_indices[mask]\n",
        "            \n",
        "            if labeled_indices.numel() >= budget:\n",
        "                break\n",
        "        \n",
        "        # Final evaluation on test set\n",
        "        final_train_subset = TensorDataset(x_pool[labeled_indices], y_pool[labeled_indices])\n",
        "        final_train_loader = DataLoader(final_train_subset, batch_size=bs, shuffle=True)\n",
        "        final_test_loader = DataLoader(test_subset, batch_size=bs, shuffle=False)\n",
        "        \n",
        "        final_model = OneHiddenMLP(input_dim=X_train_scaled.shape[1], hidden_units=hidden, output_dim=len(np.unique(y_train_val)))\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "        \n",
        "        train_passive(final_model, final_train_loader, final_test_loader, loss_fn, train_config)\n",
        "        \n",
        "        # Evaluate on test set\n",
        "        metrics = evaluate_classification(final_model, final_test_loader, device='cpu')\n",
        "        all_metrics.append(metrics)\n",
        "    \n",
        "    # Average across trials and compute std\n",
        "    final_metrics = {}\n",
        "    for key in all_metrics[0].keys():\n",
        "        values = [m[key] for m in all_metrics]\n",
        "        final_metrics[f'{key}_mean'] = float(np.mean(values))\n",
        "        final_metrics[f'{key}_std'] = float(np.std(values, ddof=1))\n",
        "    \n",
        "    return final_metrics\n",
        "\n",
        "print(\"Test set evaluation functions defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting test set evaluation...\n",
            "This will evaluate all methods on the held-out test sets that were never seen during hyperparameter tuning.\n",
            "\n",
            "Evaluating passive learning on test sets...\n",
            "  iris...\n",
            "  wine...\n",
            "  breast_cancer...\n",
            "\n",
            "Evaluating uncertainty-based active learning on test sets...\n",
            "  iris - entropy...\n",
            "  iris - margin...\n",
            "  iris - least_confidence...\n",
            "  wine - entropy...\n",
            "  wine - margin...\n",
            "  wine - least_confidence...\n",
            "  breast_cancer - entropy...\n",
            "  breast_cancer - margin...\n",
            "  breast_cancer - least_confidence...\n",
            "\n",
            "Evaluating sensitivity-based active learning on test sets...\n",
            "  iris...\n",
            "  wine...\n",
            "  breast_cancer...\n",
            "\n",
            "Test set evaluation completed!\n",
            "Results stored in test_results dictionary.\n"
          ]
        }
      ],
      "source": [
        "# Perform Test Set Evaluation\n",
        "print(\"Starting test set evaluation...\")\n",
        "print(\"This will evaluate all methods on the held-out test sets that were never seen during hyperparameter tuning.\")\n",
        "\n",
        "# Store test results\n",
        "test_results = {\n",
        "    'passive': {},\n",
        "    'uncertainty': {},\n",
        "    'sensitivity': {}\n",
        "}\n",
        "\n",
        "# Evaluate passive learning on test set\n",
        "print(\"\\nEvaluating passive learning on test sets...\")\n",
        "for dataset in DATASETS:\n",
        "    print(f\"  {dataset}...\")\n",
        "    test_results['passive'][dataset] = evaluate_passive_test(dataset, max(BUDGETS))\n",
        "\n",
        "# Evaluate uncertainty-based active learning on test set\n",
        "print(\"\\nEvaluating uncertainty-based active learning on test sets...\")\n",
        "for dataset in DATASETS:\n",
        "    test_results['uncertainty'][dataset] = {}\n",
        "    for method in UNCERTAINTY_METHODS:\n",
        "        print(f\"  {dataset} - {method}...\")\n",
        "        test_results['uncertainty'][dataset][method] = {}\n",
        "        for budget in BUDGETS:\n",
        "            test_results['uncertainty'][dataset][method][str(budget)] = evaluate_active_test(dataset, 'uncertainty', method, budget)\n",
        "\n",
        "# Evaluate sensitivity-based active learning on test set\n",
        "print(\"\\nEvaluating sensitivity-based active learning on test sets...\")\n",
        "for dataset in DATASETS:\n",
        "    print(f\"  {dataset}...\")\n",
        "    test_results['sensitivity'][dataset] = {}\n",
        "    for budget in BUDGETS:\n",
        "        test_results['sensitivity'][dataset][str(budget)] = evaluate_active_test(dataset, 'sensitivity', '', budget)\n",
        "\n",
        "print(\"\\nTest set evaluation completed!\")\n",
        "print(\"Results stored in test_results dictionary.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Comparing strategies for iris (TEST SET) ===\n",
            "Saved iris accuracy comparison (TEST SET)\n",
            "Saved iris f1_macro comparison (TEST SET)\n",
            "\n",
            "=== Comparing strategies for wine (TEST SET) ===\n",
            "Saved wine accuracy comparison (TEST SET)\n",
            "Saved wine f1_macro comparison (TEST SET)\n",
            "\n",
            "=== Comparing strategies for breast_cancer (TEST SET) ===\n",
            "Saved breast_cancer accuracy comparison (TEST SET)\n",
            "Saved breast_cancer f1_macro comparison (TEST SET)\n"
          ]
        }
      ],
      "source": [
        "# Compare all datasets using TEST SET results\n",
        "for dataset in DATASETS:\n",
        "    print(f\"\\n=== Comparing strategies for {dataset} (TEST SET) ===\")\n",
        "    \n",
        "    for metric in METRICS:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        \n",
        "        # Plot uncertainty methods\n",
        "        for method in UNCERTAINTY_METHODS:\n",
        "            means = [test_results['uncertainty'][dataset][method][str(b)][f'{metric}_mean'] for b in BUDGETS]\n",
        "            stds = [test_results['uncertainty'][dataset][method][str(b)][f'{metric}_std'] for b in BUDGETS]\n",
        "            plt.plot(BUDGETS, means, marker='o', label=f'uncertainty_{method}', linewidth=2)\n",
        "            plt.fill_between(BUDGETS, np.array(means)-np.array(stds), np.array(means)+np.array(stds), alpha=0.2)\n",
        "        \n",
        "        # Plot sensitivity method\n",
        "        means = [test_results['sensitivity'][dataset][str(b)][f'{metric}_mean'] for b in BUDGETS]\n",
        "        stds = [test_results['sensitivity'][dataset][str(b)][f'{metric}_std'] for b in BUDGETS]\n",
        "        plt.plot(BUDGETS, means, marker='s', label='sensitivity', linewidth=2)\n",
        "        plt.fill_between(BUDGETS, np.array(means)-np.array(stds), np.array(means)+np.array(stds), alpha=0.2)\n",
        "        \n",
        "        # Plot passive baseline as horizontal line\n",
        "        baseline = test_results['passive'][dataset][f'{metric}_mean']\n",
        "        plt.axhline(baseline, color='k', linestyle='--', label='passive_best', linewidth=2)\n",
        "        \n",
        "        plt.xlabel('Labeled budget (max_labels)')\n",
        "        plt.ylabel(metric)\n",
        "        plt.title(f'{dataset}: Strategies Comparison ({metric}) - TEST SET EVALUATION')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(SAVE_DIR, f'cls_{dataset}_comparison_{metric}_test.png'), dpi=200, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        \n",
        "        print(f\"Saved {dataset} {metric} comparison (TEST SET)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Summary Table (TEST SET EVALUATION) ===\n",
            "          dataset                        method  budget  accuracy_mean  \\\n",
            "0            iris                       passive     200         0.9400   \n",
            "1            iris           uncertainty_entropy     200         0.9667   \n",
            "2            iris            uncertainty_margin     200         0.9467   \n",
            "3            iris  uncertainty_least_confidence     200         0.9533   \n",
            "4            iris                   sensitivity     200         0.9600   \n",
            "5            wine                       passive     200         0.9833   \n",
            "6            wine           uncertainty_entropy     200         0.9778   \n",
            "7            wine            uncertainty_margin     200         0.9611   \n",
            "8            wine  uncertainty_least_confidence     200         0.9833   \n",
            "9            wine                   sensitivity     200         0.9778   \n",
            "10  breast_cancer                       passive     200         0.9561   \n",
            "11  breast_cancer           uncertainty_entropy     200         0.9632   \n",
            "12  breast_cancer            uncertainty_margin     200         0.9632   \n",
            "13  breast_cancer  uncertainty_least_confidence     200         0.9632   \n",
            "14  breast_cancer                   sensitivity     200         0.9632   \n",
            "\n",
            "    accuracy_std  f1_mean  f1_std  \n",
            "0         0.0149   0.9400  0.0149  \n",
            "1         0.0000   0.9666  0.0000  \n",
            "2         0.0183   0.9466  0.0182  \n",
            "3         0.0183   0.9533  0.0182  \n",
            "4         0.0149   0.9599  0.0149  \n",
            "5         0.0248   0.9823  0.0264  \n",
            "6         0.0124   0.9768  0.0130  \n",
            "7         0.0152   0.9589  0.0166  \n",
            "8         0.0152   0.9826  0.0159  \n",
            "9         0.0232   0.9765  0.0247  \n",
            "10        0.0124   0.9532  0.0129  \n",
            "11        0.0144   0.9608  0.0152  \n",
            "12        0.0144   0.9608  0.0152  \n",
            "13        0.0144   0.9608  0.0152  \n",
            "14        0.0073   0.9609  0.0077  \n",
            "\n",
            "Saved TEST SET comparison summary to ../report/figures\n",
            "All TEST SET comparison figures and summary saved!\n"
          ]
        }
      ],
      "source": [
        "# Create summary table using TEST SET results\n",
        "print(\"\\n=== Summary Table (TEST SET EVALUATION) ===\")\n",
        "summary_data = []\n",
        "\n",
        "for dataset in DATASETS:\n",
        "    # Passive learning\n",
        "    summary_data.append({\n",
        "        'dataset': dataset,\n",
        "        'method': 'passive',\n",
        "        'budget': max(BUDGETS),\n",
        "        'accuracy_mean': test_results['passive'][dataset]['accuracy_mean'],\n",
        "        'accuracy_std': test_results['passive'][dataset]['accuracy_std'],\n",
        "        'f1_mean': test_results['passive'][dataset]['f1_macro_mean'],\n",
        "        'f1_std': test_results['passive'][dataset]['f1_macro_std']\n",
        "    })\n",
        "    \n",
        "    # Uncertainty methods\n",
        "    for method in UNCERTAINTY_METHODS:\n",
        "        max_budget = str(max(BUDGETS))\n",
        "        summary_data.append({\n",
        "            'dataset': dataset,\n",
        "            'method': f'uncertainty_{method}',\n",
        "            'budget': max(BUDGETS),\n",
        "            'accuracy_mean': test_results['uncertainty'][dataset][method][max_budget]['accuracy_mean'],\n",
        "            'accuracy_std': test_results['uncertainty'][dataset][method][max_budget]['accuracy_std'],\n",
        "            'f1_mean': test_results['uncertainty'][dataset][method][max_budget]['f1_macro_mean'],\n",
        "            'f1_std': test_results['uncertainty'][dataset][method][max_budget]['f1_macro_std']\n",
        "        })\n",
        "    \n",
        "    # Sensitivity method\n",
        "    max_budget = str(max(BUDGETS))\n",
        "    summary_data.append({\n",
        "        'dataset': dataset,\n",
        "        'method': 'sensitivity',\n",
        "        'budget': max(BUDGETS),\n",
        "        'accuracy_mean': test_results['sensitivity'][dataset][max_budget]['accuracy_mean'],\n",
        "        'accuracy_std': test_results['sensitivity'][dataset][max_budget]['accuracy_std'],\n",
        "        'f1_mean': test_results['sensitivity'][dataset][max_budget]['f1_macro_mean'],\n",
        "        'f1_std': test_results['sensitivity'][dataset][max_budget]['f1_macro_std']\n",
        "    })\n",
        "\n",
        "# Convert to DataFrame for nice display\n",
        "df = pd.DataFrame(summary_data)\n",
        "print(df.round(4))\n",
        "\n",
        "# Save summary\n",
        "df.to_csv(os.path.join(SAVE_DIR, 'cls_comparison_summary_test.csv'), index=False)\n",
        "print(f'\\nSaved TEST SET comparison summary to {SAVE_DIR}')\n",
        "print('All TEST SET comparison figures and summary saved!')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Machine-Learning-441-fvr4FmDE",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
