{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Passive Classification Tuning with Cross-Validation and Multiple Trials\n",
        "import os\n",
        "import json\n",
        "import itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from typing import Dict\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "from alnn.models import OneHiddenMLP\n",
        "from alnn.training import train_passive\n",
        "from alnn.evaluation import evaluate_classification\n",
        "import torch.nn as nn\n",
        "from alnn.training import TrainConfig\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "FIGURES_DIR = os.path.join('..', 'report', 'figures')\n",
        "DATA_DIR = os.path.join('..', 'data')\n",
        "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "DATASETS = ['iris', 'wine', 'breast_cancer']\n",
        "LR = [1e-3, 3e-3, 1e-2, 3e-2]\n",
        "WD = [0.0, 1e-5, 1e-4]\n",
        "HIDDEN = [32, 64, 128]\n",
        "BS = [32, 64]\n",
        "N_TRIALS = 5  # Number of random seeds for each config\n",
        "N_FOLDS = 5  # Number of CV folds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_config_cv(dataset: str, lr: float, wd: float, hidden: int, bs: int) -> Dict[str, float]:\n",
        "    \"\"\"Evaluate a configuration using cross-validation across multiple trials.\"\"\"\n",
        "    all_metrics = []\n",
        "    \n",
        "    for trial in range(N_TRIALS):\n",
        "        # Set random seed for reproducibility\n",
        "        torch.manual_seed(42 + trial)\n",
        "        np.random.seed(42 + trial)\n",
        "        \n",
        "        trial_metrics = []\n",
        "        \n",
        "        # Load data for CV splits\n",
        "        if dataset == \"iris\":\n",
        "            ds = datasets.load_iris()\n",
        "        elif dataset == \"wine\":\n",
        "            ds = datasets.load_wine()\n",
        "        elif dataset == \"breast_cancer\":\n",
        "            ds = datasets.load_breast_cancer()\n",
        "        \n",
        "        X, y = ds.data, ds.target\n",
        "        \n",
        "        # Use StratifiedKFold for classification\n",
        "        skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42 + trial)\n",
        "        \n",
        "        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "            X_train, X_val = X[train_idx], X[val_idx]\n",
        "            y_train, y_val = y[train_idx], y[val_idx]\n",
        "            \n",
        "            # Standardize features\n",
        "            scaler = StandardScaler()\n",
        "            X_train_scaled = scaler.fit_transform(X_train)\n",
        "            X_val_scaled = scaler.transform(X_val)\n",
        "            \n",
        "            # Convert to tensors\n",
        "            X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "            y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "            X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
        "            y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
        "            \n",
        "            # Create datasets\n",
        "            train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "            val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "            \n",
        "            train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
        "            val_loader = DataLoader(val_dataset, batch_size=bs, shuffle=False)\n",
        "            \n",
        "            # Train model\n",
        "            model = OneHiddenMLP(input_dim=X_train_scaled.shape[1], hidden_units=hidden, output_dim=len(np.unique(y)))\n",
        "            loss_fn = nn.CrossEntropyLoss()\n",
        "            config = TrainConfig(learning_rate=lr, weight_decay=wd, batch_size=bs, max_epochs=200, patience=20, device='cpu')\n",
        "            \n",
        "            train_passive(model, train_loader, val_loader, loss_fn, config)\n",
        "            \n",
        "            # Evaluate\n",
        "            metrics = evaluate_classification(model, val_loader, device='cpu')\n",
        "            trial_metrics.append(metrics)\n",
        "        \n",
        "        # Average across folds for this trial\n",
        "        trial_avg = {}\n",
        "        for key in trial_metrics[0].keys():\n",
        "            trial_avg[key] = np.mean([m[key] for m in trial_metrics])\n",
        "        all_metrics.append(trial_avg)\n",
        "    \n",
        "    # Average across trials and compute std\n",
        "    final_metrics = {}\n",
        "    for key in all_metrics[0].keys():\n",
        "        values = [m[key] for m in all_metrics]\n",
        "        final_metrics[f'{key}_mean'] = float(np.mean(values))\n",
        "        final_metrics[f'{key}_std'] = float(np.std(values, ddof=1))\n",
        "    \n",
        "    return final_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "path = Path(f\"{DATA_DIR}/passive_cls_checkpoint.json\")\n",
        "with path.open() as f:\n",
        "    data = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "144"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(data[\"results\"][\"iris\"]['history'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resuming from checkpoint: 185 configs completed\n",
            "Resuming from dataset 2 (breast_cancer), config 41\n",
            "\n",
            "=== Tuning iris ===\n",
            "Skipping iris (already completed)\n",
            "\n",
            "=== Tuning wine ===\n",
            "Skipping wine (already completed)\n",
            "\n",
            "=== Tuning breast_cancer ===\n",
            "Resuming breast_cancer from config 42/72\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "breast_cancer configs:   0%|          | 0/72 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Config 1/72: lr=0.01, wd=0.0, hidden=128, bs=64\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 70\u001b[39m\n\u001b[32m     67\u001b[39m config_idx = \u001b[38;5;28mlen\u001b[39m(hist) + \u001b[32m1\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mConfig \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_configs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: lr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, wd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwd\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, hidden=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhidden\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, bs=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m res = \u001b[43mevaluate_config_cv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m res.update({\u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m: lr, \u001b[33m\"\u001b[39m\u001b[33mwd\u001b[39m\u001b[33m\"\u001b[39m: wd, \u001b[33m\"\u001b[39m\u001b[33mhidden\u001b[39m\u001b[33m\"\u001b[39m: hidden, \u001b[33m\"\u001b[39m\u001b[33mbs\u001b[39m\u001b[33m\"\u001b[39m: bs})\n\u001b[32m     72\u001b[39m hist.append(res)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mevaluate_config_cv\u001b[39m\u001b[34m(dataset, lr, wd, hidden, bs)\u001b[39m\n\u001b[32m     49\u001b[39m loss_fn = nn.CrossEntropyLoss()\n\u001b[32m     50\u001b[39m config = TrainConfig(learning_rate=lr, weight_decay=wd, batch_size=bs, max_epochs=\u001b[32m200\u001b[39m, patience=\u001b[32m20\u001b[39m, device=\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[43mtrain_passive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[32m     55\u001b[39m metrics = evaluate_classification(model, val_loader, device=\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/University/Machine-Learning-441/Assignment 3/src/alnn/training.py:50\u001b[39m, in \u001b[36mtrain_passive\u001b[39m\u001b[34m(model, train_loader, val_loader, loss_fn, config)\u001b[39m\n\u001b[32m     48\u001b[39m optimizer.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     49\u001b[39m preds = model(xb)\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m loss = \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m loss.backward()\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# Gradient clipping to prevent exploding updates\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/Machine-Learning-441-fvr4FmDE/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/Machine-Learning-441-fvr4FmDE/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/Machine-Learning-441-fvr4FmDE/lib/python3.12/site-packages/torch/nn/modules/loss.py:1310\u001b[39m, in \u001b[36mCrossEntropyLoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m   1309\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m-> \u001b[39m\u001b[32m1310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1311\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1317\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/Machine-Learning-441-fvr4FmDE/lib/python3.12/site-packages/torch/nn/functional.py:3462\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3460\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3461\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3462\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3463\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3465\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3466\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3468\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3469\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Load checkpoint if exists\n",
        "checkpoint_file = os.path.join(DATA_DIR, 'passive_cls_checkpoint.json')\n",
        "if os.path.exists(checkpoint_file):\n",
        "    with open(checkpoint_file, 'r') as f:\n",
        "        checkpoint = json.load(f)\n",
        "    print(f\"Resuming from checkpoint: {checkpoint.get('completed_configs', 0)} configs completed\")\n",
        "    BEST = checkpoint.get('results', {})\n",
        "    \n",
        "    # Determine which dataset to resume from\n",
        "    dataset_configs = len(LR) * len(WD) * len(HIDDEN) * len(BS)\n",
        "    completed_configs = checkpoint.get('completed_configs', 0)\n",
        "    \n",
        "    # Find which dataset we should resume from\n",
        "    resume_dataset_idx = completed_configs // dataset_configs\n",
        "    resume_config_idx = completed_configs % dataset_configs\n",
        "    \n",
        "    print(f\"Resuming from dataset {resume_dataset_idx} ({DATASETS[resume_dataset_idx] if resume_dataset_idx < len(DATASETS) else 'completed'}), config {resume_config_idx}\")\n",
        "    \n",
        "else:\n",
        "    checkpoint = {'completed_configs': 0, 'results': {}}\n",
        "    BEST = {}\n",
        "    resume_dataset_idx = 0\n",
        "    resume_config_idx = 0\n",
        "    print(\"Starting fresh run\")\n",
        "\n",
        "# Calculate total configs\n",
        "total_configs = len(DATASETS) * len(LR) * len(WD) * len(HIDDEN) * len(BS)\n",
        "start_time = time.time()\n",
        "\n",
        "# Process datasets starting from the resume point\n",
        "for dataset_idx, dataset in enumerate(DATASETS):\n",
        "    if dataset not in BEST:\n",
        "        BEST[dataset] = {\"best_cfg\": None, \"best_metric\": -np.inf, \"history\": []}\n",
        "    \n",
        "    print(f\"\\n=== Tuning {dataset} ===\")\n",
        "    best_metric = BEST[dataset][\"best_metric\"]\n",
        "    best_cfg = BEST[dataset][\"best_cfg\"]\n",
        "    hist = BEST[dataset][\"history\"]\n",
        "    \n",
        "    dataset_configs = len(LR) * len(WD) * len(HIDDEN) * len(BS)\n",
        "    \n",
        "    # Determine starting point for this dataset\n",
        "    if dataset_idx < resume_dataset_idx:\n",
        "        # This dataset is already completed, skip it\n",
        "        print(f\"Skipping {dataset} (already completed)\")\n",
        "        continue\n",
        "    elif dataset_idx == resume_dataset_idx:\n",
        "        # This is the dataset we need to resume from\n",
        "        start_config_idx = resume_config_idx\n",
        "        print(f\"Resuming {dataset} from config {start_config_idx + 1}/{dataset_configs}\")\n",
        "    else:\n",
        "        # This dataset hasn't been started yet\n",
        "        start_config_idx = 0\n",
        "        print(f\"Starting {dataset} from config 1/{dataset_configs}\")\n",
        "    \n",
        "    # Create progress bar for this dataset\n",
        "    pbar = tqdm(total=dataset_configs, desc=f\"{dataset} configs\", \n",
        "                initial=len(hist), position=0, leave=True)\n",
        "    \n",
        "    config_count = 0\n",
        "    for lr, wd, hidden, bs in itertools.product(LR, WD, HIDDEN, BS):\n",
        "        # Skip configs that were already completed\n",
        "        if config_count < start_config_idx:\n",
        "            config_count += 1\n",
        "            continue\n",
        "            \n",
        "        config_idx = len(hist) + 1\n",
        "        print(f'Config {config_idx}/{dataset_configs}: lr={lr}, wd={wd}, hidden={hidden}, bs={bs}')\n",
        "        \n",
        "        res = evaluate_config_cv(dataset, lr, wd, hidden, bs)\n",
        "        res.update({\"lr\": lr, \"wd\": wd, \"hidden\": hidden, \"bs\": bs})\n",
        "        hist.append(res)\n",
        "        \n",
        "        if res['accuracy_mean'] > best_metric:\n",
        "            best_metric = res['accuracy_mean']\n",
        "            best_cfg = {\"lr\": lr, \"wd\": wd, \"hidden\": hidden, \"bs\": bs}\n",
        "        \n",
        "        # Update progress bar\n",
        "        pbar.update(1)\n",
        "        pbar.set_postfix({'best_acc': f\"{best_metric:.4f}\"})\n",
        "        \n",
        "        # Save checkpoint after each config\n",
        "        checkpoint['completed_configs'] += 1\n",
        "        BEST[dataset] = {\"best_cfg\": best_cfg, \"best_metric\": best_metric, \"history\": hist}\n",
        "        \n",
        "        with open(checkpoint_file, 'w') as f:\n",
        "            json.dump(checkpoint, f, indent=2)\n",
        "        \n",
        "        config_count += 1\n",
        "    \n",
        "    pbar.close()\n",
        "    BEST[dataset] = {\"best_cfg\": best_cfg, \"best_metric\": best_metric, \"history\": hist}\n",
        "    print(f\"Best config for {dataset}: {best_cfg} (accuracy: {best_metric:.4f})\")\n",
        "\n",
        "# Save final results\n",
        "with open(os.path.join(DATA_DIR, 'passive_cls_best.json'), 'w') as f:\n",
        "    json.dump(BEST, f, indent=2)\n",
        "\n",
        "# Clean up checkpoint file\n",
        "if os.path.exists(checkpoint_file):\n",
        "    os.remove(checkpoint_file)\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(f\"\\nTotal time: {total_time/3600:.2f} hours\")\n",
        "print(f\"Average time per config: {total_time/total_configs:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot best accuracy per dataset with error bars\n",
        "plt.figure(figsize=(8, 5))\n",
        "datasets_plot = []\n",
        "means_plot = []\n",
        "stds_plot = []\n",
        "\n",
        "for dataset in DATASETS:\n",
        "    best_idx = None\n",
        "    best_acc = -np.inf\n",
        "    for i, h in enumerate(BEST[dataset]['history']):\n",
        "        if h['accuracy_mean'] > best_acc:\n",
        "            best_acc = h['accuracy_mean']\n",
        "            best_idx = i\n",
        "    \n",
        "    datasets_plot.append(dataset)\n",
        "    means_plot.append(BEST[dataset]['history'][best_idx]['accuracy_mean'])\n",
        "    stds_plot.append(BEST[dataset]['history'][best_idx]['accuracy_std'])\n",
        "\n",
        "plt.errorbar(datasets_plot, means_plot, yerr=stds_plot, fmt='o', capsize=5, capthick=2)\n",
        "plt.ylabel('Accuracy (best ± std)')\n",
        "plt.title('Passive Classification Best Accuracy (CV + Multiple Trials)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIGURES_DIR, 'passive_cls_best_accuracy.png'), dpi=200)\n",
        "plt.show()\n",
        "\n",
        "print(f'\\nSaved passive classification tuning results to {FIGURES_DIR}')\n",
        "print(f'Used {N_TRIALS} trials × {N_FOLDS} folds = {N_TRIALS * N_FOLDS} evaluations per config')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Machine-Learning-441-fvr4FmDE",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
