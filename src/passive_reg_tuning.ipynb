{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Passive Regression Tuning with Cross-Validation and Multiple Trials\n",
        "import os\n",
        "import json\n",
        "import itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "from typing import Dict\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "from alnn.models import OneHiddenMLP\n",
        "from alnn.training import train_passive\n",
        "from alnn.evaluation import evaluate_regression\n",
        "import torch.nn as nn\n",
        "from alnn.training import TrainConfig\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "SAVE_DIR = os.path.join('..', 'report', 'figures')\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "DATASETS = ['diabetes', 'linnerud', 'california']\n",
        "LR = [1e-4, 3e-4, 1e-3, 3e-3]  # More conservative LR range for regression\n",
        "WD = [0.0, 1e-5, 1e-4]\n",
        "HIDDEN = [32, 64, 128]\n",
        "BS = [32, 64]\n",
        "N_TRIALS = 5  # Number of random seeds for each config\n",
        "N_FOLDS = 5  # Number of CV folds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_config_cv(dataset: str, lr: float, wd: float, hidden: int, bs: int) -> Dict[str, float]:\n",
        "    \"\"\"Evaluate a configuration using cross-validation across multiple trials.\"\"\"\n",
        "    all_metrics = []\n",
        "    \n",
        "    for trial in range(N_TRIALS):\n",
        "        # Set random seed for reproducibility\n",
        "        torch.manual_seed(42 + trial)\n",
        "        np.random.seed(42 + trial)\n",
        "        \n",
        "        trial_metrics = []\n",
        "        \n",
        "        # Load data for CV splits\n",
        "        if dataset == \"diabetes\":\n",
        "            ds = datasets.load_diabetes()\n",
        "            y = ds.target.astype(np.float32)\n",
        "        elif dataset == \"linnerud\":\n",
        "            ds = datasets.load_linnerud()\n",
        "            y = ds.target[:, 0].astype(np.float32)  # use one target (Weight)\n",
        "        elif dataset == \"california\":\n",
        "            ds = datasets.fetch_california_housing()\n",
        "            y = ds.target.astype(np.float32)\n",
        "        \n",
        "        X = ds.data.astype(np.float32)\n",
        "        \n",
        "        # Use KFold for regression\n",
        "        kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42 + trial)\n",
        "        \n",
        "        for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
        "            X_train, X_val = X[train_idx], X[val_idx]\n",
        "            y_train, y_val = y[train_idx], y[val_idx]\n",
        "            \n",
        "            # Standardize features\n",
        "            scaler = StandardScaler()\n",
        "            X_train_scaled = scaler.fit_transform(X_train)\n",
        "            X_val_scaled = scaler.transform(X_val)\n",
        "            \n",
        "            # Convert to tensors\n",
        "            X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "            y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(-1)\n",
        "            X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
        "            y_val_tensor = torch.tensor(y_val, dtype=torch.float32).unsqueeze(-1)\n",
        "            \n",
        "            # Create datasets\n",
        "            train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "            val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "            \n",
        "            train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
        "            val_loader = DataLoader(val_dataset, batch_size=bs, shuffle=False)\n",
        "            \n",
        "            # Train model\n",
        "            model = OneHiddenMLP(input_dim=X_train_scaled.shape[1], hidden_units=hidden, output_dim=1)\n",
        "            loss_fn = nn.MSELoss()\n",
        "            config = TrainConfig(learning_rate=lr, weight_decay=wd, batch_size=bs, max_epochs=200, patience=20, device='cpu')\n",
        "            \n",
        "            train_passive(model, train_loader, val_loader, loss_fn, config)\n",
        "            \n",
        "            # Evaluate\n",
        "            metrics = evaluate_regression(model, val_loader, device='cpu')\n",
        "            trial_metrics.append(metrics)\n",
        "        \n",
        "        # Average across folds for this trial\n",
        "        trial_avg = {}\n",
        "        for key in trial_metrics[0].keys():\n",
        "            trial_avg[key] = np.mean([m[key] for m in trial_metrics])\n",
        "        all_metrics.append(trial_avg)\n",
        "    \n",
        "    # Average across trials and compute std\n",
        "    final_metrics = {}\n",
        "    for key in all_metrics[0].keys():\n",
        "        values = [m[key] for m in all_metrics]\n",
        "        final_metrics[f'{key}_mean'] = float(np.mean(values))\n",
        "        final_metrics[f'{key}_std'] = float(np.std(values, ddof=1))\n",
        "    \n",
        "    return final_metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load checkpoint if exists\n",
        "checkpoint_file = os.path.join(SAVE_DIR, 'passive_reg_checkpoint.json')\n",
        "if os.path.exists(checkpoint_file):\n",
        "    with open(checkpoint_file, 'r') as f:\n",
        "        checkpoint = json.load(f)\n",
        "    print(f\"Resuming from checkpoint: {checkpoint['completed_configs']} configs completed\")\n",
        "else:\n",
        "    checkpoint = {'completed_configs': 0, 'results': {}}\n",
        "    print(\"Starting fresh run\")\n",
        "\n",
        "BEST = checkpoint.get('results', {})\n",
        "\n",
        "# Calculate total configs\n",
        "total_configs = len(DATASETS) * len(LR) * len(WD) * len(HIDDEN) * len(BS)\n",
        "start_time = time.time()\n",
        "\n",
        "# Determine resume point\n",
        "completed_configs = checkpoint['completed_configs']\n",
        "configs_per_dataset = len(LR) * len(WD) * len(HIDDEN) * len(BS)\n",
        "resume_dataset_idx = completed_configs // configs_per_dataset\n",
        "resume_config_idx = completed_configs % configs_per_dataset\n",
        "\n",
        "print(f\"Resuming from dataset {resume_dataset_idx}, config {resume_config_idx}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process datasets starting from the resume point\n",
        "for dataset_idx, dataset in enumerate(DATASETS):\n",
        "    if dataset not in BEST:\n",
        "        BEST[dataset] = {\"best_cfg\": None, \"best_metric\": np.inf, \"history\": []}\n",
        "    \n",
        "    print(f\"\\n=== Tuning {dataset} ===\")\n",
        "    best_metric = BEST[dataset][\"best_metric\"]\n",
        "    best_cfg = BEST[dataset][\"best_cfg\"]\n",
        "    hist = BEST[dataset][\"history\"]\n",
        "    \n",
        "    dataset_configs = len(LR) * len(WD) * len(HIDDEN) * len(BS)\n",
        "    \n",
        "    # Determine starting point for this dataset\n",
        "    if dataset_idx < resume_dataset_idx:\n",
        "        # This dataset is already completed, skip it\n",
        "        print(f\"Skipping {dataset} (already completed)\")\n",
        "        continue\n",
        "    elif dataset_idx == resume_dataset_idx:\n",
        "        # This is the dataset we need to resume from\n",
        "        start_config_idx = resume_config_idx\n",
        "        print(f\"Resuming {dataset} from config {start_config_idx + 1}/{dataset_configs}\")\n",
        "    else:\n",
        "        # This dataset hasn't been started yet\n",
        "        start_config_idx = 0\n",
        "        print(f\"Starting {dataset} from config 1/{dataset_configs}\")\n",
        "    \n",
        "    # Create progress bar for this dataset\n",
        "    pbar = tqdm(total=dataset_configs, desc=f\"{dataset} configs\", \n",
        "                initial=len(hist), position=0, leave=True)\n",
        "    \n",
        "    config_count = 0\n",
        "    for lr, wd, hidden, bs in itertools.product(LR, WD, HIDDEN, BS):\n",
        "        # Skip configurations that have already been processed\n",
        "        if config_count < start_config_idx:\n",
        "            config_count += 1\n",
        "            continue\n",
        "            \n",
        "        config_idx = len(hist) + 1\n",
        "        print(f'Config {config_idx}/{dataset_configs}: lr={lr}, wd={wd}, hidden={hidden}, bs={bs}')\n",
        "        \n",
        "        res = evaluate_config_cv(dataset, lr, wd, hidden, bs)\n",
        "        res.update({\"lr\": lr, \"wd\": wd, \"hidden\": hidden, \"bs\": bs})\n",
        "        hist.append(res)\n",
        "        \n",
        "        if res['rmse_mean'] < best_metric:\n",
        "            best_metric = res['rmse_mean']\n",
        "            best_cfg = {\"lr\": lr, \"wd\": wd, \"hidden\": hidden, \"bs\": bs}\n",
        "        \n",
        "        # Update progress bar\n",
        "        pbar.update(1)\n",
        "        pbar.set_postfix({'best_rmse': f\"{best_metric:.4f}\"})\n",
        "        \n",
        "        # Save checkpoint after each config\n",
        "        checkpoint['completed_configs'] += 1\n",
        "        BEST[dataset] = {\"best_cfg\": best_cfg, \"best_metric\": best_metric, \"history\": hist}\n",
        "        \n",
        "        with open(checkpoint_file, 'w') as f:\n",
        "            json.dump(checkpoint, f, indent=2)\n",
        "        \n",
        "        config_count += 1\n",
        "    \n",
        "    pbar.close()\n",
        "    BEST[dataset] = {\"best_cfg\": best_cfg, \"best_metric\": best_metric, \"history\": hist}\n",
        "    print(f\"Best config for {dataset}: {best_cfg} (RMSE: {best_metric:.4f})\")\n",
        "\n",
        "# Save final results\n",
        "with open(os.path.join(SAVE_DIR, 'passive_reg_best.json'), 'w') as f:\n",
        "    json.dump(BEST, f, indent=2)\n",
        "\n",
        "# Clean up checkpoint file\n",
        "if os.path.exists(checkpoint_file):\n",
        "    os.remove(checkpoint_file)\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(f\"\\nTotal time: {total_time/3600:.2f} hours\")\n",
        "print(f\"Average time per config: {total_time/total_configs:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot best RMSE per dataset with error bars\n",
        "plt.figure(figsize=(8, 5))\n",
        "datasets_plot = []\n",
        "means_plot = []\n",
        "stds_plot = []\n",
        "\n",
        "for dataset_idx, dataset in enumerate(DATASETS):\n",
        "    best_idx = None\n",
        "    best_rmse = np.inf\n",
        "    for i, h in enumerate(BEST[dataset]['history']):\n",
        "        if h['rmse_mean'] < best_rmse:\n",
        "            best_rmse = h['rmse_mean']\n",
        "            best_idx = i\n",
        "    \n",
        "    datasets_plot.append(dataset)\n",
        "    means_plot.append(BEST[dataset]['history'][best_idx]['rmse_mean'])\n",
        "    stds_plot.append(BEST[dataset]['history'][best_idx]['rmse_std'])\n",
        "\n",
        "plt.errorbar(datasets_plot, means_plot, yerr=stds_plot, fmt='o', capsize=5, capthick=2)\n",
        "plt.ylabel('RMSE (best ± std)')\n",
        "plt.title('Passive Regression Best RMSE (CV + Multiple Trials)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(SAVE_DIR, 'passive_reg_best_rmse.png'), dpi=200)\n",
        "plt.show()\n",
        "\n",
        "print(f'\\nSaved passive regression tuning results to {SAVE_DIR}')\n",
        "print(f'Used {N_TRIALS} trials × {N_FOLDS} folds = {N_TRIALS * N_FOLDS} evaluations per config')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Machine-Learning-441-fvr4FmDE",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
