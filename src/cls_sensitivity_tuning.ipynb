{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Active Learning Exploration - Classification (Sensitivity) with Cross-Validation\n",
        "import os\n",
        "import json\n",
        "import itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Dict, List\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "from alnn.experiments import ActiveConfig, run_active_classification\n",
        "from alnn.training import TrainConfig\n",
        "\n",
        "SAVE_DIR = os.path.join('..', 'report', 'figures')\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "DATASETS = ['iris', 'wine', 'breast_cancer']\n",
        "BUDGETS = [40, 80, 120, 160, 200]\n",
        "N_TRIALS = 5  # Number of random seeds for each config\n",
        "\n",
        "# Tuning grids\n",
        "LRS = [1e-3, 3e-3, 1e-2]\n",
        "WDS = [0.0, 1e-5, 1e-4]\n",
        "HIDDENS = [32, 64, 128]\n",
        "BSS = [32, 64]\n",
        "INITS = [10, 20, 40]\n",
        "QUERIES = [5, 10, 20]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tune_hparams(dataset: str, tune_budget: int) -> tuple:\n",
        "    \"\"\"Tune hyperparameters using cross-validation.\"\"\"\n",
        "    best_acc = -float('inf')\n",
        "    best_config = None\n",
        "    \n",
        "    total_configs = len(LRS) * len(WDS) * len(HIDDENS) * len(BSS) * len(INITS) * len(QUERIES)\n",
        "    \n",
        "    # Load checkpoint if exists\n",
        "    checkpoint_file = os.path.join(SAVE_DIR, f'cls_sensitivity_{dataset}_checkpoint.json')\n",
        "    if os.path.exists(checkpoint_file):\n",
        "        with open(checkpoint_file, 'r') as f:\n",
        "            checkpoint = json.load(f)\n",
        "        print(f\"Resuming hyperparameter tuning from checkpoint: {checkpoint['completed_configs']} configs completed\")\n",
        "        best_acc = checkpoint.get('best_acc', -float('inf'))\n",
        "        best_config = checkpoint.get('best_config', None)\n",
        "        completed_configs = checkpoint['completed_configs']\n",
        "    else:\n",
        "        checkpoint = {'completed_configs': 0, 'best_acc': -float('inf'), 'best_config': None}\n",
        "        completed_configs = 0\n",
        "        print(\"Starting fresh hyperparameter tuning\")\n",
        "    \n",
        "    # Create progress bar\n",
        "    pbar = tqdm(total=total_configs, desc=f\"Tuning {dataset}-sensitivity\", \n",
        "                initial=completed_configs, position=0, leave=True)\n",
        "    \n",
        "    config_idx = completed_configs\n",
        "    for lr, wd, hidden, bs, init, query in itertools.product(LRS, WDS, HIDDENS, BSS, INITS, QUERIES):\n",
        "        if config_idx < completed_configs:\n",
        "            config_idx += 1\n",
        "            pbar.update(1)\n",
        "            continue\n",
        "            \n",
        "        print(f'Tuning config {config_idx+1}/{total_configs}: lr={lr}, wd={wd}, hidden={hidden}, bs={bs}, init={init}, query={query}')\n",
        "        \n",
        "        # Evaluate this configuration\n",
        "        metrics = []\n",
        "        for seed in range(N_TRIALS):\n",
        "            torch.manual_seed(42 + seed)\n",
        "            tcfg = TrainConfig(learning_rate=lr, weight_decay=wd, batch_size=bs, max_epochs=200, patience=20, device='cpu')\n",
        "            acfg = ActiveConfig(initial_labeled=init, query_batch=query, max_labels=tune_budget, device='cpu')\n",
        "            res = run_active_classification(dataset_name=dataset, strategy='sensitivity', \n",
        "                                          hidden_units=hidden, train_config=tcfg, active_config=acfg)\n",
        "            metrics.append(res['accuracy'])\n",
        "        \n",
        "        avg_acc = np.mean(metrics)\n",
        "        \n",
        "        if avg_acc > best_acc:\n",
        "            best_acc = avg_acc\n",
        "            best_config = (TrainConfig(learning_rate=lr, weight_decay=wd, batch_size=bs, max_epochs=200, patience=20, device='cpu'),\n",
        "                          ActiveConfig(initial_labeled=init, query_batch=query, max_labels=tune_budget, device='cpu'),\n",
        "                          hidden)\n",
        "        \n",
        "        # Update progress bar\n",
        "        pbar.update(1)\n",
        "        pbar.set_postfix({'best_acc': f\"{best_acc:.4f}\"})\n",
        "        \n",
        "        # Save checkpoint after each config\n",
        "        checkpoint['completed_configs'] = config_idx + 1\n",
        "        checkpoint['best_acc'] = best_acc\n",
        "        checkpoint['best_config'] = best_config\n",
        "        with open(checkpoint_file, 'w') as f:\n",
        "            json.dump(checkpoint, f, indent=2)\n",
        "        \n",
        "        config_idx += 1\n",
        "    \n",
        "    pbar.close()\n",
        "    \n",
        "    # Clean up checkpoint file\n",
        "    if os.path.exists(checkpoint_file):\n",
        "        os.remove(checkpoint_file)\n",
        "    \n",
        "    print(f\"Best config for {dataset}-sensitivity: accuracy={best_acc:.4f}\")\n",
        "    return best_config\n",
        "\n",
        "\n",
        "def evaluate_curve(dataset: str, budgets: List[int]) -> Dict[int, Dict[str, float]]:\n",
        "    \"\"\"Evaluate active learning curve using best hyperparameters.\"\"\"\n",
        "    tune_budget = sorted(budgets)[len(budgets)//2]\n",
        "    tcfg, acfg_base, hidden_units = tune_hparams(dataset, tune_budget)\n",
        "\n",
        "    # Load checkpoint if exists\n",
        "    checkpoint_file = os.path.join(SAVE_DIR, f'cls_sensitivity_{dataset}_curve_checkpoint.json')\n",
        "    if os.path.exists(checkpoint_file):\n",
        "        with open(checkpoint_file, 'r') as f:\n",
        "            checkpoint = json.load(f)\n",
        "        print(f\"Resuming curve evaluation from checkpoint: {len(checkpoint['results'])} budgets completed\")\n",
        "        results = checkpoint['results']\n",
        "    else:\n",
        "        checkpoint = {'results': {}}\n",
        "        results = {}\n",
        "        print(\"Starting fresh curve evaluation\")\n",
        "    \n",
        "    # Create progress bar\n",
        "    pbar = tqdm(total=len(budgets), desc=f\"Curve {dataset}-sensitivity\", \n",
        "                initial=len(results), position=0, leave=True)\n",
        "    \n",
        "    for max_labels in budgets:\n",
        "        if str(max_labels) in results:\n",
        "            pbar.update(1)\n",
        "            continue\n",
        "            \n",
        "        print(f'Evaluating {dataset}-sensitivity at budget {max_labels}')\n",
        "        metrics = []\n",
        "        for seed in range(N_TRIALS):\n",
        "            torch.manual_seed(42 + seed)\n",
        "            acfg = ActiveConfig(initial_labeled=acfg_base.initial_labeled, query_batch=acfg_base.query_batch, max_labels=max_labels, device=acfg_base.device)\n",
        "            res = run_active_classification(dataset_name=dataset, strategy='sensitivity', hidden_units=hidden_units, train_config=tcfg, active_config=acfg)\n",
        "            metrics.append(res)\n",
        "        \n",
        "        keys = metrics[0].keys()\n",
        "        results[str(max_labels)] = {f'{k}_mean': float(np.mean([m[k] for m in metrics])) for k in keys}\n",
        "        results[str(max_labels)].update({f'{k}_std': float(np.std([m[k] for m in metrics], ddof=1)) for k in keys})\n",
        "        \n",
        "        # Update progress bar\n",
        "        pbar.update(1)\n",
        "        \n",
        "        # Save checkpoint after each budget\n",
        "        checkpoint['results'] = results\n",
        "        with open(checkpoint_file, 'w') as f:\n",
        "            json.dump(checkpoint, f, indent=2)\n",
        "    \n",
        "    pbar.close()\n",
        "    \n",
        "    # Clean up checkpoint file\n",
        "    if os.path.exists(checkpoint_file):\n",
        "        os.remove(checkpoint_file)\n",
        "    \n",
        "    return {int(k): v for k, v in results.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load checkpoint if exists\n",
        "checkpoint_file = os.path.join(SAVE_DIR, 'cls_sensitivity_main_checkpoint.json')\n",
        "if os.path.exists(checkpoint_file):\n",
        "    with open(checkpoint_file, 'r') as f:\n",
        "        checkpoint = json.load(f)\n",
        "    print(f\"Resuming from checkpoint: {checkpoint['completed_datasets']} datasets completed\")\n",
        "    all_results = checkpoint.get('results', {})\n",
        "else:\n",
        "    checkpoint = {'completed_datasets': 0, 'results': {}}\n",
        "    all_results = {}\n",
        "    print(\"Starting fresh run\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for dataset in DATASETS:\n",
        "    if dataset in all_results:\n",
        "        print(f\"\\n=== Skipping {dataset} (already completed) ===\")\n",
        "        continue\n",
        "        \n",
        "    print(f\"\\n=== Processing {dataset} ===\")\n",
        "    curve = evaluate_curve(dataset, BUDGETS)\n",
        "    all_results[dataset] = curve\n",
        "    \n",
        "    # Plot curves\n",
        "    for metric in ['accuracy', 'f1_macro']:\n",
        "        budgets = sorted(curve.keys())\n",
        "        means = [curve[b][f'{metric}_mean'] for b in budgets]\n",
        "        stds = [curve[b][f'{metric}_std'] for b in budgets]\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        plt.plot(budgets, means, marker='o', label='sensitivity', linewidth=2)\n",
        "        plt.fill_between(budgets, np.array(means)-np.array(stds), np.array(means)+np.array(stds), alpha=0.2)\n",
        "        plt.xlabel('Labeled budget (max_labels)')\n",
        "        plt.ylabel(metric)\n",
        "        plt.title(f'{dataset} - sensitivity ({metric}) - CV + Multiple Trials')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.legend()\n",
        "        fname = f'cls_{dataset}_sensitivity_{metric}.png'\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(SAVE_DIR, fname), dpi=200)\n",
        "        plt.close()\n",
        "    \n",
        "    # Save checkpoint after each dataset\n",
        "    checkpoint['completed_datasets'] += 1\n",
        "    checkpoint['results'] = all_results\n",
        "    with open(checkpoint_file, 'w') as f:\n",
        "        json.dump(checkpoint, f, indent=2)\n",
        "\n",
        "# Save final results\n",
        "with open(os.path.join(SAVE_DIR, 'cls_sensitivity_results.json'), 'w') as f:\n",
        "    json.dump(all_results, f, indent=2)\n",
        "\n",
        "# Clean up checkpoint file\n",
        "if os.path.exists(checkpoint_file):\n",
        "    os.remove(checkpoint_file)\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(f\"\\nTotal time: {total_time/3600:.2f} hours\")\n",
        "print(f'\\nSaved figures and results to {SAVE_DIR}')\n",
        "print(f'Used {N_TRIALS} trials per config')\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
