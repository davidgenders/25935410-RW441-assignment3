{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare Active Learning Strategies - Regression (with CV + Multiple Trials)\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "SAVE_DIR = os.path.join('..', 'report', 'figures')\n",
        "DATA_DIR = os.path.join('..', 'data')\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# Load results (these should be generated by the updated notebooks)\n",
        "try:\n",
        "    with open(os.path.join(DATA_DIR, 'reg_uncertainty_results.json'), 'r') as f:\n",
        "        unc = json.load(f)\n",
        "    with open(os.path.join(DATA_DIR, 'reg_sensitivity_results.json'), 'r') as f:\n",
        "        sen = json.load(f)\n",
        "    with open(os.path.join(DATA_DIR, 'passive_reg_best.json'), 'r') as f:\n",
        "        pas = json.load(f)\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Results file not found: {e}\")\n",
        "    print(\"Please run the updated exploration notebooks first to generate results with CV + multiple trials\")\n",
        "    exit()\n",
        "\n",
        "DATASETS = ['diabetes', 'linnerud', 'california']\n",
        "METRICS = ['rmse', 'mae', 'r2']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Comparing strategies for diabetes ===\n",
            "Saved diabetes rmse comparison\n",
            "Saved diabetes mae comparison\n",
            "Saved diabetes r2 comparison\n",
            "\n",
            "=== Comparing strategies for linnerud ===\n",
            "Saved linnerud rmse comparison\n",
            "Saved linnerud mae comparison\n",
            "Saved linnerud r2 comparison\n",
            "\n",
            "=== Comparing strategies for california ===\n",
            "Saved california rmse comparison\n",
            "Saved california mae comparison\n",
            "Saved california r2 comparison\n"
          ]
        }
      ],
      "source": [
        "# Compare all datasets\n",
        "for dataset in DATASETS:\n",
        "    print(f\"\\n=== Comparing strategies for {dataset} ===\")\n",
        "    \n",
        "    # Get budgets and methods\n",
        "    budgets = sorted([int(b) for b in next(iter(unc[dataset].values())).keys()])\n",
        "    methods = list(unc[dataset].keys())\n",
        "    \n",
        "    for metric in METRICS:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        \n",
        "        # Plot uncertainty methods\n",
        "        for m in methods:\n",
        "            means = [unc[dataset][m][str(b)][f'{metric}_mean'] for b in budgets]\n",
        "            stds = [unc[dataset][m][str(b)][f'{metric}_std'] for b in budgets]\n",
        "            plt.plot(budgets, means, marker='o', label=f'uncertainty_{m}', linewidth=2)\n",
        "            plt.fill_between(budgets, np.array(means)-np.array(stds), np.array(means)+np.array(stds), alpha=0.2)\n",
        "        \n",
        "        # Plot sensitivity method\n",
        "        if dataset in sen:\n",
        "            means = [sen[dataset][str(b)][f'{metric}_mean'] for b in budgets]\n",
        "            stds = [sen[dataset][str(b)][f'{metric}_std'] for b in budgets]\n",
        "            plt.plot(budgets, means, marker='s', label='sensitivity', linewidth=2)\n",
        "            plt.fill_between(budgets, np.array(means)-np.array(stds), np.array(means)+np.array(stds), alpha=0.2)\n",
        "        \n",
        "        # Plot passive baseline as horizontal line (lower is better for RMSE/MAE, higher for R2)\n",
        "        baseline = pas[dataset]['best_metric']\n",
        "        plt.axhline(baseline, color='k', linestyle='--', label='passive_best', linewidth=2)\n",
        "        \n",
        "        plt.xlabel('Labeled budget (max_labels)')\n",
        "        plt.ylabel(metric)\n",
        "        plt.title(f'{dataset}: Strategies Comparison ({metric}) - CV + Multiple Trials')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(SAVE_DIR, f'reg_{dataset}_comparison_{metric}.png'), dpi=200, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        \n",
        "        print(f\"Saved {dataset} {metric} comparison\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Summary Table ===\n",
            "       dataset            method  budget  rmse_mean  rmse_std  mae_mean  \\\n",
            "0     diabetes           entropy     200    53.1361    1.0090   42.6384   \n",
            "1     diabetes            margin     200    53.1361    1.0090   42.6384   \n",
            "2     diabetes  least_confidence     200    53.1361    1.0090   42.6384   \n",
            "3     diabetes       sensitivity     200    53.5369    0.9533   43.2594   \n",
            "4     linnerud           entropy     200    34.1819    3.8368   28.9691   \n",
            "5     linnerud            margin     200    34.1819    3.8368   28.9691   \n",
            "6     linnerud  least_confidence     200    34.1819    3.8368   28.9691   \n",
            "7     linnerud       sensitivity     200    34.1819    3.8368   28.9691   \n",
            "8   california           entropy     200     1.1526    0.2946    0.7096   \n",
            "9   california            margin     200     1.1526    0.2946    0.7096   \n",
            "10  california  least_confidence     200     1.1526    0.2946    0.7096   \n",
            "11  california       sensitivity     200     1.0039    0.2210    0.6331   \n",
            "\n",
            "    mae_std  r2_mean  r2_std  \n",
            "0    1.5306   0.4669  0.0202  \n",
            "1    1.5306   0.4669  0.0202  \n",
            "2    1.5306   0.4669  0.0202  \n",
            "3    1.2314   0.4589  0.0192  \n",
            "4    3.6587  -3.1971  0.9703  \n",
            "5    3.6587  -3.1971  0.9703  \n",
            "6    3.6587  -3.1971  0.9703  \n",
            "7    3.6587  -3.1971  0.9703  \n",
            "8    0.1295  -0.0667  0.5384  \n",
            "9    0.1295  -0.0667  0.5384  \n",
            "10   0.1295  -0.0667  0.5384  \n",
            "11   0.1338   0.2011  0.3478  \n",
            "\n",
            "Saved comparison summary to ../report/figures\n",
            "All comparison figures and summary saved!\n"
          ]
        }
      ],
      "source": [
        "# Create summary table\n",
        "print(\"\\n=== Summary Table ===\")\n",
        "summary_data = []\n",
        "\n",
        "for dataset in DATASETS:\n",
        "    # Get budgets and methods\n",
        "    budgets = sorted([int(b) for b in next(iter(unc[dataset].values())).keys()])\n",
        "    methods = list(unc[dataset].keys())\n",
        "    \n",
        "    for method in methods + ['sensitivity']:\n",
        "        if method == 'sensitivity' and dataset not in sen:\n",
        "            continue\n",
        "            \n",
        "        if method == 'sensitivity':\n",
        "            data = sen[dataset]\n",
        "        else:\n",
        "            data = unc[dataset][method]\n",
        "        \n",
        "        # Get performance at highest budget\n",
        "        max_budget = max(budgets)\n",
        "        rmse_mean = data[str(max_budget)]['rmse_mean']\n",
        "        rmse_std = data[str(max_budget)]['rmse_std']\n",
        "        mae_mean = data[str(max_budget)]['mae_mean']\n",
        "        mae_std = data[str(max_budget)]['mae_std']\n",
        "        r2_mean = data[str(max_budget)]['r2_mean']\n",
        "        r2_std = data[str(max_budget)]['r2_std']\n",
        "        \n",
        "        summary_data.append({\n",
        "            'dataset': dataset,\n",
        "            'method': method,\n",
        "            'budget': max_budget,\n",
        "            'rmse_mean': rmse_mean,\n",
        "            'rmse_std': rmse_std,\n",
        "            'mae_mean': mae_mean,\n",
        "            'mae_std': mae_std,\n",
        "            'r2_mean': r2_mean,\n",
        "            'r2_std': r2_std\n",
        "        })\n",
        "\n",
        "# Convert to DataFrame for nice display\n",
        "df = pd.DataFrame(summary_data)\n",
        "print(df.round(4))\n",
        "\n",
        "# Save summary\n",
        "df.to_csv(os.path.join(SAVE_DIR, 'reg_comparison_summary.csv'), index=False)\n",
        "print(f'\\nSaved comparison summary to {SAVE_DIR}')\n",
        "print('All comparison figures and summary saved!')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Machine-Learning-441-fvr4FmDE",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
