{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from nn.models import OneHiddenMLP\n",
        "from nn.training import train_passive, TrainConfig\n",
        "from nn.evaluation import evaluate_regression\n",
        "from nn.experiments import ActiveConfig, run_active_regression\n",
        "from nn.strategies import uncertainty_sampling, sensitivity_sampling, UncertaintySamplingConfig\n",
        "\n",
        "from typing import Dict\n",
        "\n",
        "SAVE_DIR = os.path.join('..', 'report', 'figures')\n",
        "DATA_DIR = os.path.join('..', 'data')\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "with open(os.path.join(DATA_DIR, 'reg_uncertainty_results.json'), 'r') as f:\n",
        "    unc_results = json.load(f)\n",
        "with open(os.path.join(DATA_DIR, 'reg_sensitivity_results.json'), 'r') as f:\n",
        "    sen_results = json.load(f)\n",
        "with open(os.path.join(DATA_DIR, 'passive_reg_best.json'), 'r') as f:\n",
        "    pas_results = json.load(f)\n",
        "\n",
        "DATASETS = ['diabetes', 'wine_quality', 'california']\n",
        "METRICS = ['rmse', 'mae', 'r2']\n",
        "UNCERTAINTY_METHODS = ['entropy', 'margin', 'least_confidence']\n",
        "ACTIVE_PARAMS = {\n",
        "    'diabetes': {'init': 20, 'query': 10, 'budget': 150},\n",
        "    'wine_quality': {'init': 30, 'query': 15, 'budget': 300},\n",
        "    'california': {'init': 50, 'query': 20, 'budget': 1000}\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_data_splits_regression(dataset: str):\n",
        "    # Load data\n",
        "    if dataset == \"diabetes\":\n",
        "        ds = datasets.load_diabetes()\n",
        "        y = ds.target.astype(np.float32)\n",
        "    elif dataset == \"wine_quality\":\n",
        "        from sklearn.datasets import fetch_openml\n",
        "        ds = fetch_openml('wine-quality-red', version=1, as_frame=False, parser='auto')\n",
        "        X = ds.data.astype(np.float32)\n",
        "        y = ds.target.astype(np.float32)\n",
        "    elif dataset == \"california\":\n",
        "        ds = datasets.fetch_california_housing()\n",
        "        y = ds.target.astype(np.float32)\n",
        "        X = ds.data.astype(np.float32)\n",
        "    \n",
        "    # Only set X here if it wasn't already set above\n",
        "    if dataset != \"wine_quality\":\n",
        "        X = ds.data.astype(np.float32)\n",
        "    \n",
        "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "    \n",
        "    return X_train_val, X_test, y_train_val, y_test\n",
        "\n",
        "def evaluate_passive_test_regression(dataset: str) -> Dict[str, float]:\n",
        "    # Get best hyperparameters from tuning results\n",
        "    best_cfg = pas_results[dataset]['best_cfg']\n",
        "    lr = best_cfg['lr']\n",
        "    wd = best_cfg['wd']\n",
        "    hidden = best_cfg['hidden']\n",
        "    bs = best_cfg['bs']\n",
        "    \n",
        "    # Get data splits\n",
        "    X_train_val, X_test, y_train_val, y_test = get_data_splits_regression(dataset)\n",
        "    \n",
        "    all_metrics = []\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_val)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    \n",
        "    # Convert to tensors\n",
        "    X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train_val, dtype=torch.float32).unsqueeze(-1)\n",
        "    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(-1)\n",
        "    \n",
        "    # Create datasets\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "    \n",
        "    train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=bs, shuffle=False)\n",
        "    \n",
        "    # Train model\n",
        "    model = OneHiddenMLP(input_dim=X_train_scaled.shape[1], hidden_units=hidden, output_dim=1)\n",
        "    loss_fn = nn.MSELoss()\n",
        "    config = TrainConfig(learning_rate=lr, weight_decay=wd, batch_size=bs, max_epochs=200, patience=20)\n",
        "    \n",
        "    train_passive(model, train_loader, test_loader, loss_fn, config)\n",
        "    \n",
        "    # Evaluate on test set\n",
        "    metrics = evaluate_regression(model, test_loader)\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "def evaluate_active_test_regression(dataset: str, strategy: str, method: str, budget: int) -> Dict[str, float]:\n",
        "    dataset_params = ACTIVE_PARAMS[dataset]\n",
        "    init = dataset_params['inits']\n",
        "    query = dataset_params['queries']\n",
        "    hidden, bs = 64, 64\n",
        "    if strategy == 'uncertainty':\n",
        "        best_cfg = unc_results[dataset][method]['best_cfg']['train_config']\n",
        "        lr = best_cfg['learning_rate']\n",
        "        wd = best_cfg['weight_decay']\n",
        "    else:\n",
        "        best_cfg = sen_results[dataset]['best_cfg']['train_config']\n",
        "        lr = best_cfg['learning_rate']\n",
        "        wd = best_cfg['weight_decay']\n",
        "\n",
        "    # Get data splits\n",
        "    X_train_val, X_test, y_train_val, y_test = get_data_splits_regression(dataset)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_val)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    \n",
        "    # Convert to tensors\n",
        "    X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train_val, dtype=torch.float32).unsqueeze(-1)\n",
        "    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(-1)\n",
        "    \n",
        "    # Create datasets\n",
        "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "    test_loader = DataLoader(test_dataset, batch_size=bs, shuffle=False)\n",
        "    \n",
        "    # Simulate active learning on the train+val set\n",
        "    train_config = TrainConfig(learning_rate=lr, weight_decay=wd, batch_size=bs, \n",
        "                                max_epochs=200, patience=20)\n",
        "    \n",
        "    # Create initial labeled pool\n",
        "    num_train = X_train_scaled.shape[0]\n",
        "    labeled_indices = torch.randperm(num_train)[:init]\n",
        "    unlabeled_indices = torch.tensor([i for i in range(num_train) if i not in labeled_indices.tolist()], dtype=torch.long)\n",
        "    \n",
        "    x_pool = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "    y_pool = y_train_tensor.clone()\n",
        "    \n",
        "    # Active learning loop\n",
        "    while labeled_indices.numel() < min(budget, num_train):\n",
        "        # Train model on current labeled set\n",
        "        train_subset = TensorDataset(x_pool[labeled_indices], y_pool[labeled_indices])\n",
        "        \n",
        "        train_loader = DataLoader(train_subset, batch_size=bs, shuffle=True)\n",
        "        \n",
        "        model = OneHiddenMLP(input_dim=X_train_scaled.shape[1], hidden_units=hidden, output_dim=1)\n",
        "        loss_fn = nn.MSELoss()\n",
        "        \n",
        "        train_passive(model, train_loader, test_loader, loss_fn, train_config)\n",
        "        \n",
        "        if unlabeled_indices.numel() == 0:\n",
        "            break\n",
        "        \n",
        "        # Query selection\n",
        "        if strategy == 'uncertainty':\n",
        "            sel = uncertainty_sampling(\n",
        "                model,\n",
        "                x_pool[unlabeled_indices].to('cpu'),\n",
        "                query,\n",
        "                UncertaintySamplingConfig(mode=\"regression\", method=method),\n",
        "            )\n",
        "        elif strategy == 'sensitivity':\n",
        "            sel = sensitivity_sampling(model, x_pool[unlabeled_indices].to(train_config.device), query)\n",
        "        \n",
        "        # Update labeled and unlabeled sets\n",
        "        newly_selected = unlabeled_indices[sel]\n",
        "        labeled_indices = torch.unique(torch.cat([labeled_indices, newly_selected]))\n",
        "        mask = torch.ones_like(unlabeled_indices, dtype=torch.bool)\n",
        "        mask[sel] = False\n",
        "        unlabeled_indices = unlabeled_indices[mask]\n",
        "        \n",
        "        if labeled_indices.numel() >= budget:\n",
        "            break\n",
        "    \n",
        "    # Final evaluation on test set\n",
        "    final_train_subset = TensorDataset(x_pool[labeled_indices], y_pool[labeled_indices])\n",
        "    final_train_loader = DataLoader(final_train_subset, batch_size=bs, shuffle=True)\n",
        "    final_test_loader = DataLoader(test_dataset, batch_size=bs, shuffle=False)\n",
        "    \n",
        "    final_model = OneHiddenMLP(input_dim=X_train_scaled.shape[1], hidden_units=hidden, output_dim=1)\n",
        "    loss_fn = nn.MSELoss()\n",
        "    \n",
        "    train_passive(final_model, final_train_loader, final_test_loader, loss_fn, train_config)\n",
        "    \n",
        "    # Evaluate on test set\n",
        "    metrics = evaluate_regression(final_model, final_test_loader, device='cpu')\n",
        "    \n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_results = {\n",
        "    'passive': {},\n",
        "    'uncertainty': {},\n",
        "    'sensitivity': {}\n",
        "}\n",
        "\n",
        "# Evaluate passive learning on test set\n",
        "for dataset in DATASETS:\n",
        "    print(f\"Running passive on {dataset}\")\n",
        "    test_results['passive'][dataset] = evaluate_passive_test_regression(dataset)\n",
        "\n",
        "# Evaluate uncertainty-based active learning on test set\n",
        "for dataset in DATASETS:\n",
        "    budget = ACTIVE_PARAMS[dataset]['budget']\n",
        "    test_results['uncertainty'][dataset] = {}\n",
        "    for method in UNCERTAINTY_METHODS:\n",
        "        print(f\"Running uncertainty on {dataset} - {method}\")\n",
        "        test_results['uncertainty'][dataset][method] = {}\n",
        "        test_results['uncertainty'][dataset][method][str(budget)] = evaluate_active_test_regression(dataset, 'uncertainty', method, budget)\n",
        "\n",
        "# Evaluate sensitivity-based active learning on test set\n",
        "for dataset in DATASETS:\n",
        "    budget = ACTIVE_PARAMS[dataset]['budget']\n",
        "    print(f\"Running sensitivity on {dataset}...\")\n",
        "    test_results['sensitivity'][dataset] = {}\n",
        "    test_results['sensitivity'][dataset][str(budget)] = evaluate_active_test_regression(dataset, 'sensitivity', '', budget)\n",
        "\n",
        "print(\"Done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for dataset in DATASETS:\n",
        "    b = ACTIVE_PARAMS[dataset]['budget']\n",
        "    for metric in METRICS:\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        \n",
        "        # Plot uncertainty methods\n",
        "        for method in UNCERTAINTY_METHODS:\n",
        "            result = [test_results['uncertainty'][dataset][method][str(b)][f'{metric}']]\n",
        "        \n",
        "        # Plot sensitivity method\n",
        "        result = [test_results['sensitivity'][dataset][str(b)][f'{metric}']]\n",
        "        \n",
        "        # Plot passive baseline as horizontal line\n",
        "        baseline = test_results['passive'][dataset][f'{metric}']\n",
        "        plt.axhline(baseline, color='k', linestyle='--', label='passive_best', linewidth=2)\n",
        "        \n",
        "        plt.xlabel('Labeled budget (max_labels)')\n",
        "        plt.ylabel(metric)\n",
        "        plt.title(f'{dataset}: Strategies Comparison ({metric}) - TEST SET EVALUATION')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(SAVE_DIR, f'reg_{dataset}_comparison_{metric}_test.png'), dpi=200, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        \n",
        "        print(f\"Saved {dataset} {metric} comparison (TEST SET)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary_data = []\n",
        "\n",
        "for dataset in DATASETS:\n",
        "\n",
        "    budget = ACTIVE_PARAMS[dataset]['budget']\n",
        "    # Passive learning\n",
        "    summary_data.append({\n",
        "        'dataset': dataset,\n",
        "        'method': 'passive',\n",
        "        'budget': budget,\n",
        "        'rmse': test_results['passive'][dataset]['rmse'],\n",
        "        'mae': test_results['passive'][dataset]['mae'],\n",
        "        'r2': test_results['passive'][dataset]['r2'],\n",
        "    })\n",
        "    \n",
        "    # Uncertainty methods\n",
        "    for method in UNCERTAINTY_METHODS:\n",
        "        max_budget = str(budget)\n",
        "        summary_data.append({\n",
        "            'dataset': dataset,\n",
        "            'method': f'uncertainty_{method}',\n",
        "            'budget': budget,\n",
        "            'rmse': test_results['uncertainty'][dataset][method][max_budget]['rmse'],\n",
        "            'mae': test_results['uncertainty'][dataset][method][max_budget]['mae'],\n",
        "            'r2': test_results['uncertainty'][dataset][method][max_budget]['r2'],\n",
        "        })\n",
        "    \n",
        "    # Sensitivity method\n",
        "    max_budget = str(budget)\n",
        "    summary_data.append({\n",
        "        'dataset': dataset,\n",
        "        'method': 'sensitivity',\n",
        "        'budget': budget,\n",
        "        'rmse': test_results['sensitivity'][dataset][max_budget]['rmse'],\n",
        "        'mae': test_results['sensitivity'][dataset][max_budget]['mae'],\n",
        "        'r2': test_results['sensitivity'][dataset][max_budget]['r2'],\n",
        "    })\n",
        "\n",
        "# Convert to DataFrame for nice display\n",
        "df = pd.DataFrame(summary_data)\n",
        "print(df.round(4))\n",
        "\n",
        "# Save summary\n",
        "df.to_csv(os.path.join(SAVE_DIR, 'reg_comparison_summary_test.csv'), index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Machine-Learning-441-fvr4FmDE",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
