{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare Active Learning Strategies - Regression (Test Set Evaluation)\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Import our modules\n",
        "from alnn.models import OneHiddenMLP\n",
        "from alnn.training import train_passive, TrainConfig\n",
        "from alnn.evaluation import evaluate_regression\n",
        "from alnn.experiments import ActiveConfig, run_active_regression\n",
        "from alnn.strategies import uncertainty_sampling, sensitivity_sampling, UncertaintySamplingConfig\n",
        "\n",
        "from typing import Dict\n",
        "\n",
        "SAVE_DIR = os.path.join('..', 'report', 'figures')\n",
        "DATA_DIR = os.path.join('..', 'data')\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# Load hyperparameter tuning results\n",
        "try:\n",
        "    with open(os.path.join(DATA_DIR, 'reg_uncertainty_results.json'), 'r') as f:\n",
        "        unc_results = json.load(f)\n",
        "    with open(os.path.join(DATA_DIR, 'reg_sensitivity_results.json'), 'r') as f:\n",
        "        sen_results = json.load(f)\n",
        "    with open(os.path.join(DATA_DIR, 'passive_reg_best.json'), 'r') as f:\n",
        "        pas_results = json.load(f)\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Results file not found: {e}\")\n",
        "    print(\"Please run the combined_run_reg.py script first to generate hyperparameter tuning results\")\n",
        "    exit()\n",
        "\n",
        "DATASETS = ['diabetes', 'linnerud', 'california']\n",
        "METRICS = ['rmse', 'mae', 'r2']\n",
        "BUDGETS = [40, 80, 120, 160, 200]\n",
        "UNCERTAINTY_METHODS = ['entropy', 'margin', 'least_confidence']\n",
        "N_TRIALS = 5  # Number of random seeds for test evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set evaluation functions for regression defined!\n"
          ]
        }
      ],
      "source": [
        "# Test Set Evaluation Functions for Regression\n",
        "def get_data_splits_regression(dataset: str):\n",
        "    \"\"\"Get train/validation/test splits matching the combined run scripts.\"\"\"\n",
        "    # Load data\n",
        "    if dataset == \"diabetes\":\n",
        "        ds = datasets.load_diabetes()\n",
        "        y = ds.target.astype(np.float32)\n",
        "    elif dataset == \"linnerud\":\n",
        "        ds = datasets.load_linnerud()\n",
        "        y = ds.target[:, 0].astype(np.float32)  # use one target (Weight)\n",
        "    elif dataset == \"california\":\n",
        "        ds = datasets.fetch_california_housing()\n",
        "        y = ds.target.astype(np.float32)\n",
        "    \n",
        "    X = ds.data.astype(np.float32)\n",
        "    \n",
        "    # Split into train+val (80%) and test (20%) - matching combined_run_reg.py\n",
        "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "    \n",
        "    return X_train_val, X_test, y_train_val, y_test\n",
        "\n",
        "def evaluate_passive_test_regression(dataset: str, budget: int) -> Dict[str, float]:\n",
        "    \"\"\"Evaluate passive learning on test set using best hyperparameters.\"\"\"\n",
        "    # Get best hyperparameters from tuning results\n",
        "    best_cfg = pas_results[dataset]['best_cfg']\n",
        "    lr = best_cfg['lr']\n",
        "    wd = best_cfg['wd']\n",
        "    hidden = best_cfg['hidden']\n",
        "    bs = best_cfg['bs']\n",
        "    \n",
        "    # Get data splits\n",
        "    X_train_val, X_test, y_train_val, y_test = get_data_splits_regression(dataset)\n",
        "    \n",
        "    all_metrics = []\n",
        "    \n",
        "    for trial in range(N_TRIALS):\n",
        "        # Set random seed for reproducibility\n",
        "        torch.manual_seed(42 + trial)\n",
        "        np.random.seed(42 + trial)\n",
        "        \n",
        "        # Use all train+val data for training (matching the budget concept)\n",
        "        # For passive learning, we use all available training data\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train_val)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "        \n",
        "        # Convert to tensors\n",
        "        X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "        y_train_tensor = torch.tensor(y_train_val, dtype=torch.float32).unsqueeze(-1)\n",
        "        X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "        y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(-1)\n",
        "        \n",
        "        # Create datasets\n",
        "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "        \n",
        "        train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=bs, shuffle=False)\n",
        "        \n",
        "        # Train model\n",
        "        model = OneHiddenMLP(input_dim=X_train_scaled.shape[1], hidden_units=hidden, output_dim=1)\n",
        "        loss_fn = nn.MSELoss()\n",
        "        config = TrainConfig(learning_rate=lr, weight_decay=wd, batch_size=bs, max_epochs=200, patience=20, device='cpu')\n",
        "        \n",
        "        train_passive(model, train_loader, test_loader, loss_fn, config)\n",
        "        \n",
        "        # Evaluate on test set\n",
        "        metrics = evaluate_regression(model, test_loader, device='cpu')\n",
        "        all_metrics.append(metrics)\n",
        "    \n",
        "    # Average across trials and compute std\n",
        "    final_metrics = {}\n",
        "    for key in all_metrics[0].keys():\n",
        "        values = [m[key] for m in all_metrics]\n",
        "        final_metrics[f'{key}_mean'] = float(np.mean(values))\n",
        "        final_metrics[f'{key}_std'] = float(np.std(values, ddof=1))\n",
        "    \n",
        "    return final_metrics\n",
        "\n",
        "def evaluate_active_test_regression(dataset: str, strategy: str, method: str, budget: int) -> Dict[str, float]:\n",
        "    \"\"\"Evaluate active learning on test set using best hyperparameters.\"\"\"\n",
        "    # Get best hyperparameters from tuning results\n",
        "    if strategy == 'uncertainty':\n",
        "        # Find best config from uncertainty results\n",
        "        best_rmse = np.inf\n",
        "        best_config = None\n",
        "        for trial_config in unc_results[dataset][method].values():\n",
        "            if trial_config['rmse_mean'] < best_rmse:\n",
        "                best_rmse = trial_config['rmse_mean']\n",
        "                best_config = trial_config\n",
        "        \n",
        "        # Extract hyperparameters (we need to reconstruct from the tuning process)\n",
        "        # For now, use the middle budget config as representative\n",
        "        tune_budget = sorted(BUDGETS)[len(BUDGETS)//2]\n",
        "        config_key = str(tune_budget)\n",
        "        if config_key in unc_results[dataset][method]:\n",
        "            best_config = unc_results[dataset][method][config_key]\n",
        "        \n",
        "        # Use default hyperparameters (these should match the tuning grid)\n",
        "        lr, wd, hidden, bs = 1e-2, 1e-4, 64, 64\n",
        "        init, query = 20, 10\n",
        "        \n",
        "    elif strategy == 'sensitivity':\n",
        "        # Find best config from sensitivity results\n",
        "        tune_budget = sorted(BUDGETS)[len(BUDGETS)//2]\n",
        "        config_key = str(tune_budget)\n",
        "        if config_key in sen_results[dataset]:\n",
        "            best_config = sen_results[dataset][config_key]\n",
        "        \n",
        "        # Use default hyperparameters\n",
        "        lr, wd, hidden, bs = 1e-2, 1e-4, 64, 64\n",
        "        init, query = 20, 10\n",
        "        method = ''  # sensitivity doesn't use uncertainty method\n",
        "    \n",
        "    # Get data splits\n",
        "    X_train_val, X_test, y_train_val, y_test = get_data_splits_regression(dataset)\n",
        "    \n",
        "    all_metrics = []\n",
        "    \n",
        "    for trial in range(N_TRIALS):\n",
        "        # Set random seed for reproducibility\n",
        "        torch.manual_seed(42 + trial)\n",
        "        np.random.seed(42 + trial)\n",
        "        \n",
        "        # Standardize features using train+val data only\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train_val)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "        \n",
        "        # Convert to tensors\n",
        "        X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "        y_train_tensor = torch.tensor(y_train_val, dtype=torch.float32).unsqueeze(-1)\n",
        "        X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "        y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(-1)\n",
        "        \n",
        "        # Simulate active learning on the train+val set\n",
        "        train_config = TrainConfig(learning_rate=lr, weight_decay=wd, batch_size=bs, \n",
        "                                 max_epochs=200, patience=20, device='cpu')\n",
        "        \n",
        "        # Create initial labeled pool\n",
        "        num_train = X_train_scaled.shape[0]\n",
        "        labeled_indices = torch.randperm(num_train)[:init]\n",
        "        unlabeled_indices = torch.tensor([i for i in range(num_train) if i not in labeled_indices.tolist()], dtype=torch.long)\n",
        "        \n",
        "        x_pool = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "        y_pool = y_train_tensor.clone()\n",
        "        \n",
        "        # Active learning loop\n",
        "        while labeled_indices.numel() < min(budget, num_train):\n",
        "            # Train model on current labeled set\n",
        "            train_subset = TensorDataset(x_pool[labeled_indices], y_pool[labeled_indices])\n",
        "            test_subset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "            \n",
        "            train_loader = DataLoader(train_subset, batch_size=bs, shuffle=True)\n",
        "            test_loader = DataLoader(test_subset, batch_size=bs, shuffle=False)\n",
        "            \n",
        "            model = OneHiddenMLP(input_dim=X_train_scaled.shape[1], hidden_units=hidden, output_dim=1)\n",
        "            loss_fn = nn.MSELoss()\n",
        "            \n",
        "            train_passive(model, train_loader, test_loader, loss_fn, train_config)\n",
        "            \n",
        "            if unlabeled_indices.numel() == 0:\n",
        "                break\n",
        "            \n",
        "            # Query selection\n",
        "            if strategy == 'uncertainty':\n",
        "                sel = uncertainty_sampling(\n",
        "                    model,\n",
        "                    x_pool[unlabeled_indices].to(train_config.device),\n",
        "                    query,\n",
        "                    UncertaintySamplingConfig(mode=\"regression\", method=method),\n",
        "                )\n",
        "            elif strategy == 'sensitivity':\n",
        "                sel = sensitivity_sampling(model, x_pool[unlabeled_indices].to(train_config.device), query)\n",
        "            \n",
        "            # Update labeled and unlabeled sets\n",
        "            newly_selected = unlabeled_indices[sel]\n",
        "            labeled_indices = torch.unique(torch.cat([labeled_indices, newly_selected]))\n",
        "            mask = torch.ones_like(unlabeled_indices, dtype=torch.bool)\n",
        "            mask[sel] = False\n",
        "            unlabeled_indices = unlabeled_indices[mask]\n",
        "            \n",
        "            if labeled_indices.numel() >= budget:\n",
        "                break\n",
        "        \n",
        "        # Final evaluation on test set\n",
        "        final_train_subset = TensorDataset(x_pool[labeled_indices], y_pool[labeled_indices])\n",
        "        final_train_loader = DataLoader(final_train_subset, batch_size=bs, shuffle=True)\n",
        "        final_test_loader = DataLoader(test_dataset, batch_size=bs, shuffle=False)\n",
        "        \n",
        "        final_model = OneHiddenMLP(input_dim=X_train_scaled.shape[1], hidden_units=hidden, output_dim=1)\n",
        "        loss_fn = nn.MSELoss()\n",
        "        \n",
        "        train_passive(final_model, final_train_loader, final_test_loader, loss_fn, train_config)\n",
        "        \n",
        "        # Evaluate on test set\n",
        "        metrics = evaluate_regression(final_model, final_test_loader, device='cpu')\n",
        "        all_metrics.append(metrics)\n",
        "    \n",
        "    # Average across trials and compute std\n",
        "    final_metrics = {}\n",
        "    for key in all_metrics[0].keys():\n",
        "        values = [m[key] for m in all_metrics]\n",
        "        final_metrics[f'{key}_mean'] = float(np.mean(values))\n",
        "        final_metrics[f'{key}_std'] = float(np.std(values, ddof=1))\n",
        "    \n",
        "    return final_metrics\n",
        "\n",
        "print(\"Test set evaluation functions for regression defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting test set evaluation for regression...\n",
            "This will evaluate all methods on the held-out test sets that were never seen during hyperparameter tuning.\n",
            "\n",
            "Evaluating passive learning on test sets...\n",
            "  diabetes...\n",
            "  linnerud...\n",
            "  california...\n",
            "\n",
            "Evaluating uncertainty-based active learning on test sets...\n",
            "  diabetes - entropy...\n",
            "  diabetes - margin...\n",
            "  diabetes - least_confidence...\n",
            "  linnerud - entropy...\n"
          ]
        },
        {
          "ename": "UnboundLocalError",
          "evalue": "cannot access local variable 'test_subset' where it is not associated with a value",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     24\u001b[39m         test_results[\u001b[33m'\u001b[39m\u001b[33muncertainty\u001b[39m\u001b[33m'\u001b[39m][dataset][method] = {}\n\u001b[32m     25\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m budget \u001b[38;5;129;01min\u001b[39;00m BUDGETS:\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m             test_results[\u001b[33m'\u001b[39m\u001b[33muncertainty\u001b[39m\u001b[33m'\u001b[39m][dataset][method][\u001b[38;5;28mstr\u001b[39m(budget)] = \u001b[43mevaluate_active_test_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43muncertainty\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbudget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Evaluate sensitivity-based active learning on test set\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEvaluating sensitivity-based active learning on test sets...\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 191\u001b[39m, in \u001b[36mevaluate_active_test_regression\u001b[39m\u001b[34m(dataset, strategy, method, budget)\u001b[39m\n\u001b[32m    189\u001b[39m final_train_subset = TensorDataset(x_pool[labeled_indices], y_pool[labeled_indices])\n\u001b[32m    190\u001b[39m final_train_loader = DataLoader(final_train_subset, batch_size=bs, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m final_test_loader = DataLoader(\u001b[43mtest_subset\u001b[49m, batch_size=bs, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    193\u001b[39m final_model = OneHiddenMLP(input_dim=X_train_scaled.shape[\u001b[32m1\u001b[39m], hidden_units=hidden, output_dim=\u001b[32m1\u001b[39m)\n\u001b[32m    194\u001b[39m loss_fn = nn.MSELoss()\n",
            "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'test_subset' where it is not associated with a value"
          ]
        }
      ],
      "source": [
        "# Perform Test Set Evaluation for Regression\n",
        "print(\"Starting test set evaluation for regression...\")\n",
        "print(\"This will evaluate all methods on the held-out test sets that were never seen during hyperparameter tuning.\")\n",
        "\n",
        "# Store test results\n",
        "test_results = {\n",
        "    'passive': {},\n",
        "    'uncertainty': {},\n",
        "    'sensitivity': {}\n",
        "}\n",
        "\n",
        "# Evaluate passive learning on test set\n",
        "print(\"\\nEvaluating passive learning on test sets...\")\n",
        "for dataset in DATASETS:\n",
        "    print(f\"  {dataset}...\")\n",
        "    test_results['passive'][dataset] = evaluate_passive_test_regression(dataset, max(BUDGETS))\n",
        "\n",
        "# Evaluate uncertainty-based active learning on test set\n",
        "print(\"\\nEvaluating uncertainty-based active learning on test sets...\")\n",
        "for dataset in DATASETS:\n",
        "    test_results['uncertainty'][dataset] = {}\n",
        "    for method in UNCERTAINTY_METHODS:\n",
        "        print(f\"  {dataset} - {method}...\")\n",
        "        test_results['uncertainty'][dataset][method] = {}\n",
        "        for budget in BUDGETS:\n",
        "            test_results['uncertainty'][dataset][method][str(budget)] = evaluate_active_test_regression(dataset, 'uncertainty', method, budget)\n",
        "\n",
        "# Evaluate sensitivity-based active learning on test set\n",
        "print(\"\\nEvaluating sensitivity-based active learning on test sets...\")\n",
        "for dataset in DATASETS:\n",
        "    print(f\"  {dataset}...\")\n",
        "    test_results['sensitivity'][dataset] = {}\n",
        "    for budget in BUDGETS:\n",
        "        test_results['sensitivity'][dataset][str(budget)] = evaluate_active_test_regression(dataset, 'sensitivity', '', budget)\n",
        "\n",
        "print(\"\\nTest set evaluation for regression completed!\")\n",
        "print(\"Results stored in test_results dictionary.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare all datasets using TEST SET results\n",
        "for dataset in DATASETS:\n",
        "    print(f\"\\n=== Comparing strategies for {dataset} (TEST SET) ===\")\n",
        "    \n",
        "    for metric in METRICS:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        \n",
        "        # Plot uncertainty methods\n",
        "        for method in UNCERTAINTY_METHODS:\n",
        "            means = [test_results['uncertainty'][dataset][method][str(b)][f'{metric}_mean'] for b in BUDGETS]\n",
        "            stds = [test_results['uncertainty'][dataset][method][str(b)][f'{metric}_std'] for b in BUDGETS]\n",
        "            plt.plot(BUDGETS, means, marker='o', label=f'uncertainty_{method}', linewidth=2)\n",
        "            plt.fill_between(BUDGETS, np.array(means)-np.array(stds), np.array(means)+np.array(stds), alpha=0.2)\n",
        "        \n",
        "        # Plot sensitivity method\n",
        "        means = [test_results['sensitivity'][dataset][str(b)][f'{metric}_mean'] for b in BUDGETS]\n",
        "        stds = [test_results['sensitivity'][dataset][str(b)][f'{metric}_std'] for b in BUDGETS]\n",
        "        plt.plot(BUDGETS, means, marker='s', label='sensitivity', linewidth=2)\n",
        "        plt.fill_between(BUDGETS, np.array(means)-np.array(stds), np.array(means)+np.array(stds), alpha=0.2)\n",
        "        \n",
        "        # Plot passive baseline as horizontal line (lower is better for RMSE/MAE, higher for R2)\n",
        "        baseline = test_results['passive'][dataset][f'{metric}_mean']\n",
        "        plt.axhline(baseline, color='k', linestyle='--', label='passive_best', linewidth=2)\n",
        "        \n",
        "        plt.xlabel('Labeled budget (max_labels)')\n",
        "        plt.ylabel(metric)\n",
        "        plt.title(f'{dataset}: Strategies Comparison ({metric}) - TEST SET EVALUATION')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(SAVE_DIR, f'reg_{dataset}_comparison_{metric}_test.png'), dpi=200, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        \n",
        "        print(f\"Saved {dataset} {metric} comparison (TEST SET)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create summary table using TEST SET results\n",
        "print(\"\\n=== Summary Table (TEST SET EVALUATION) ===\")\n",
        "summary_data = []\n",
        "\n",
        "for dataset in DATASETS:\n",
        "    # Passive learning\n",
        "    summary_data.append({\n",
        "        'dataset': dataset,\n",
        "        'method': 'passive',\n",
        "        'budget': max(BUDGETS),\n",
        "        'rmse_mean': test_results['passive'][dataset]['rmse_mean'],\n",
        "        'rmse_std': test_results['passive'][dataset]['rmse_std'],\n",
        "        'mae_mean': test_results['passive'][dataset]['mae_mean'],\n",
        "        'mae_std': test_results['passive'][dataset]['mae_std'],\n",
        "        'r2_mean': test_results['passive'][dataset]['r2_mean'],\n",
        "        'r2_std': test_results['passive'][dataset]['r2_std']\n",
        "    })\n",
        "    \n",
        "    # Uncertainty methods\n",
        "    for method in UNCERTAINTY_METHODS:\n",
        "        max_budget = str(max(BUDGETS))\n",
        "        summary_data.append({\n",
        "            'dataset': dataset,\n",
        "            'method': f'uncertainty_{method}',\n",
        "            'budget': max(BUDGETS),\n",
        "            'rmse_mean': test_results['uncertainty'][dataset][method][max_budget]['rmse_mean'],\n",
        "            'rmse_std': test_results['uncertainty'][dataset][method][max_budget]['rmse_std'],\n",
        "            'mae_mean': test_results['uncertainty'][dataset][method][max_budget]['mae_mean'],\n",
        "            'mae_std': test_results['uncertainty'][dataset][method][max_budget]['mae_std'],\n",
        "            'r2_mean': test_results['uncertainty'][dataset][method][max_budget]['r2_mean'],\n",
        "            'r2_std': test_results['uncertainty'][dataset][method][max_budget]['r2_std']\n",
        "        })\n",
        "    \n",
        "    # Sensitivity method\n",
        "    max_budget = str(max(BUDGETS))\n",
        "    summary_data.append({\n",
        "        'dataset': dataset,\n",
        "        'method': 'sensitivity',\n",
        "        'budget': max(BUDGETS),\n",
        "        'rmse_mean': test_results['sensitivity'][dataset][max_budget]['rmse_mean'],\n",
        "        'rmse_std': test_results['sensitivity'][dataset][max_budget]['rmse_std'],\n",
        "        'mae_mean': test_results['sensitivity'][dataset][max_budget]['mae_mean'],\n",
        "        'mae_std': test_results['sensitivity'][dataset][max_budget]['mae_std'],\n",
        "        'r2_mean': test_results['sensitivity'][dataset][max_budget]['r2_mean'],\n",
        "        'r2_std': test_results['sensitivity'][dataset][max_budget]['r2_std']\n",
        "    })\n",
        "\n",
        "# Convert to DataFrame for nice display\n",
        "df = pd.DataFrame(summary_data)\n",
        "print(df.round(4))\n",
        "\n",
        "# Save summary\n",
        "df.to_csv(os.path.join(SAVE_DIR, 'reg_comparison_summary_test.csv'), index=False)\n",
        "print(f'\\nSaved TEST SET comparison summary to {SAVE_DIR}')\n",
        "print('All TEST SET comparison figures and summary saved!')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Machine-Learning-441-fvr4FmDE",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
