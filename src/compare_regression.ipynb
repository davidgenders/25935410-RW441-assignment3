{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from nn.models import OneHiddenMLP\n",
        "from nn.training import train_passive, TrainConfig\n",
        "from nn.evaluation import evaluate_regression\n",
        "from nn.experiments import ActiveConfig, run_active_regression\n",
        "from nn.strategies import uncertainty_sampling, sensitivity_sampling, UncertaintySamplingConfig\n",
        "\n",
        "from typing import Dict\n",
        "\n",
        "SAVE_DIR = os.path.join('..', 'report', 'figures')\n",
        "DATA_DIR = os.path.join('..', 'data')\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "with open(os.path.join(DATA_DIR, 'reg_sensitivity_results.json'), 'r') as f:\n",
        "    sen_results = json.load(f)\n",
        "with open(os.path.join(DATA_DIR, 'passive_reg_best.json'), 'r') as f:\n",
        "    pas_results = json.load(f)\n",
        "\n",
        "DATASETS = ['diabetes', 'wine_quality', 'california']\n",
        "METRICS = ['rmse', 'mae', 'r2']\n",
        "UNCERTAINTY_METHODS = ['entropy', 'margin', 'least_confidence']\n",
        "ACTIVE_PARAMS = {\n",
        "    'diabetes': {'init': 20, 'query': 10, 'budget': 150},\n",
        "    'wine_quality': {'init': 30, 'query': 15, 'budget': 300},\n",
        "    'california': {'init': 50, 'query': 20, 'budget': 1000}\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_data_splits_regression(dataset: str):\n",
        "    # Load data\n",
        "    if dataset == \"diabetes\":\n",
        "        ds = datasets.load_diabetes()\n",
        "        y = ds.target.astype(np.float32)\n",
        "    elif dataset == \"wine_quality\":\n",
        "        from sklearn.datasets import fetch_openml\n",
        "        ds = fetch_openml('wine-quality-red', version=1, as_frame=False, parser='auto')\n",
        "        X = ds.data.astype(np.float32)\n",
        "        y = ds.target.astype(np.float32)\n",
        "    elif dataset == \"california\":\n",
        "        ds = datasets.fetch_california_housing()\n",
        "        y = ds.target.astype(np.float32)\n",
        "        X = ds.data.astype(np.float32)\n",
        "    \n",
        "    # Only set X here if it wasn't already set above\n",
        "    if dataset != \"wine_quality\":\n",
        "        X = ds.data.astype(np.float32)\n",
        "    \n",
        "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "    \n",
        "    return X_train_val, X_test, y_train_val, y_test\n",
        "\n",
        "def evaluate_passive_test_regression(dataset: str) -> Dict[str, float]:\n",
        "    # Get best hyperparameters from tuning results\n",
        "    best_cfg = pas_results[dataset]['best_cfg']\n",
        "    lr = best_cfg['lr']\n",
        "    wd = best_cfg['wd']\n",
        "    hidden = best_cfg['hidden']\n",
        "    bs = best_cfg['bs']\n",
        "    \n",
        "    # Get data splits\n",
        "    X_train_val, X_test, y_train_val, y_test = get_data_splits_regression(dataset)\n",
        "    \n",
        "    all_metrics = []\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_val)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    \n",
        "    # Convert to tensors\n",
        "    X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train_val, dtype=torch.float32).unsqueeze(-1)\n",
        "    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(-1)\n",
        "    \n",
        "    # Create datasets\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "    \n",
        "    train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=bs, shuffle=False)\n",
        "    \n",
        "    # Train model\n",
        "    model = OneHiddenMLP(input_dim=X_train_scaled.shape[1], hidden_units=hidden, output_dim=1)\n",
        "    loss_fn = nn.MSELoss()\n",
        "    config = TrainConfig(learning_rate=lr, weight_decay=wd, batch_size=bs, max_epochs=200, patience=20)\n",
        "    \n",
        "    train_passive(model, train_loader, test_loader, loss_fn, config)\n",
        "    \n",
        "    # Evaluate on test set\n",
        "    metrics = evaluate_regression(model, test_loader)\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "def evaluate_active_test_regression(dataset: str, strategy: str, method: str, budget: int) -> Dict[str, float]:\n",
        "    dataset_params = ACTIVE_PARAMS[dataset]\n",
        "    init = dataset_params['init']\n",
        "    query = dataset_params['query']\n",
        "    hidden, bs = 64, 64\n",
        "    if strategy == 'uncertainty':\n",
        "        lr = 0.0003\n",
        "        wd = 0.0001\n",
        "    else:\n",
        "        best_cfg = sen_results[dataset]['best_cfg']['train_config']\n",
        "        lr = best_cfg['learning_rate']\n",
        "        wd = best_cfg['weight_decay']\n",
        "\n",
        "    # Get data splits\n",
        "    X_train_val, X_test, y_train_val, y_test = get_data_splits_regression(dataset)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_val)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    \n",
        "    # Convert to tensors\n",
        "    X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train_val, dtype=torch.float32).unsqueeze(-1)\n",
        "    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(-1)\n",
        "    \n",
        "    # Create datasets\n",
        "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "    test_loader = DataLoader(test_dataset, batch_size=bs, shuffle=False)\n",
        "    \n",
        "    # Simulate active learning on the train+val set\n",
        "    train_config = TrainConfig(learning_rate=lr, weight_decay=wd, batch_size=bs, \n",
        "                                max_epochs=200, patience=20)\n",
        "    \n",
        "    # Create initial labeled pool\n",
        "    num_train = X_train_scaled.shape[0]\n",
        "    labeled_indices = torch.randperm(num_train)[:init]\n",
        "    unlabeled_indices = torch.tensor([i for i in range(num_train) if i not in labeled_indices.tolist()], dtype=torch.long)\n",
        "    \n",
        "    x_pool = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "    y_pool = y_train_tensor.clone()\n",
        "    \n",
        "    # Active learning loop\n",
        "    while labeled_indices.numel() < min(budget, num_train):\n",
        "        # Train model on current labeled set\n",
        "        train_subset = TensorDataset(x_pool[labeled_indices], y_pool[labeled_indices])\n",
        "        \n",
        "        train_loader = DataLoader(train_subset, batch_size=bs, shuffle=True)\n",
        "        \n",
        "        model = OneHiddenMLP(input_dim=X_train_scaled.shape[1], hidden_units=hidden, output_dim=1)\n",
        "        loss_fn = nn.MSELoss()\n",
        "        \n",
        "        train_passive(model, train_loader, test_loader, loss_fn, train_config)\n",
        "        \n",
        "        if unlabeled_indices.numel() == 0:\n",
        "            break\n",
        "        \n",
        "        # Query selection\n",
        "        if strategy == 'uncertainty':\n",
        "            sel = uncertainty_sampling(\n",
        "                model,\n",
        "                x_pool[unlabeled_indices].to('cpu'),\n",
        "                query,\n",
        "                UncertaintySamplingConfig(mode=\"regression\", method=method),\n",
        "            )\n",
        "        elif strategy == 'sensitivity':\n",
        "            sel = sensitivity_sampling(model, x_pool[unlabeled_indices].to('cpu'), query)\n",
        "        \n",
        "        # Update labeled and unlabeled sets\n",
        "        newly_selected = unlabeled_indices[sel]\n",
        "        labeled_indices = torch.unique(torch.cat([labeled_indices, newly_selected]))\n",
        "        mask = torch.ones_like(unlabeled_indices, dtype=torch.bool)\n",
        "        mask[sel] = False\n",
        "        unlabeled_indices = unlabeled_indices[mask]\n",
        "        \n",
        "        if labeled_indices.numel() >= budget:\n",
        "            break\n",
        "    \n",
        "    # Final evaluation on test set\n",
        "    final_train_subset = TensorDataset(x_pool[labeled_indices], y_pool[labeled_indices])\n",
        "    final_train_loader = DataLoader(final_train_subset, batch_size=bs, shuffle=True)\n",
        "    final_test_loader = DataLoader(test_dataset, batch_size=bs, shuffle=False)\n",
        "    \n",
        "    final_model = OneHiddenMLP(input_dim=X_train_scaled.shape[1], hidden_units=hidden, output_dim=1)\n",
        "    loss_fn = nn.MSELoss()\n",
        "    \n",
        "    train_passive(final_model, final_train_loader, final_test_loader, loss_fn, train_config)\n",
        "    \n",
        "    # Evaluate on test set\n",
        "    metrics = evaluate_regression(final_model, final_test_loader)\n",
        "    \n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running passive on diabetes\n",
            "Running passive on wine_quality\n",
            "Running passive on california\n",
            "Running uncertainty on diabetes - entropy\n",
            "Running uncertainty on diabetes - margin\n",
            "Running uncertainty on diabetes - least_confidence\n",
            "Running uncertainty on wine_quality - entropy\n",
            "Running uncertainty on wine_quality - margin\n",
            "Running uncertainty on wine_quality - least_confidence\n",
            "Running uncertainty on california - entropy\n",
            "Running uncertainty on california - margin\n",
            "Running uncertainty on california - least_confidence\n",
            "Running sensitivity on diabetes...\n",
            "Running sensitivity on wine_quality...\n",
            "Running sensitivity on california...\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "test_results = {\n",
        "    'passive': {},\n",
        "    'uncertainty': {},\n",
        "    'sensitivity': {}\n",
        "}\n",
        "\n",
        "# Evaluate passive learning on test set\n",
        "for dataset in DATASETS:\n",
        "    print(f\"Running passive on {dataset}\")\n",
        "    test_results['passive'][dataset] = evaluate_passive_test_regression(dataset)\n",
        "\n",
        "# Evaluate uncertainty-based active learning on test set\n",
        "for dataset in DATASETS:\n",
        "    budget = ACTIVE_PARAMS[dataset]['budget']\n",
        "    test_results['uncertainty'][dataset] = {}\n",
        "    for method in UNCERTAINTY_METHODS:\n",
        "        print(f\"Running uncertainty on {dataset} - {method}\")\n",
        "        test_results['uncertainty'][dataset][method] = {}\n",
        "        test_results['uncertainty'][dataset][method][str(budget)] = evaluate_active_test_regression(dataset, 'uncertainty', method, budget)\n",
        "\n",
        "# Evaluate sensitivity-based active learning on test set\n",
        "for dataset in DATASETS:\n",
        "    budget = ACTIVE_PARAMS[dataset]['budget']\n",
        "    print(f\"Running sensitivity on {dataset}...\")\n",
        "    test_results['sensitivity'][dataset] = {}\n",
        "    test_results['sensitivity'][dataset][str(budget)] = evaluate_active_test_regression(dataset, 'sensitivity', '', budget)\n",
        "\n",
        "print(\"Done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         dataset                        method  budget      rmse       mae  \\\n",
            "0       diabetes                       passive     150   51.3046   40.7117   \n",
            "1       diabetes           uncertainty_entropy     150  151.3289  133.6237   \n",
            "2       diabetes            uncertainty_margin     150  153.4836  134.9409   \n",
            "3       diabetes  uncertainty_least_confidence     150  153.0798  135.4649   \n",
            "4       diabetes                   sensitivity     150  153.3196  137.1308   \n",
            "5   wine_quality                       passive     300    0.6021    0.4754   \n",
            "6   wine_quality           uncertainty_entropy     300    1.7657    1.3952   \n",
            "7   wine_quality            uncertainty_margin     300    2.1148    1.7154   \n",
            "8   wine_quality  uncertainty_least_confidence     300    2.0495    1.6283   \n",
            "9   wine_quality                   sensitivity     300    1.2974    1.0186   \n",
            "10    california                       passive    1000    0.5372    0.3632   \n",
            "11    california           uncertainty_entropy    1000    0.7750    0.5424   \n",
            "12    california            uncertainty_margin    1000    0.7681    0.5388   \n",
            "13    california  uncertainty_least_confidence    1000    0.7673    0.5279   \n",
            "14    california                   sensitivity    1000    0.8877    0.6355   \n",
            "\n",
            "        r2  \n",
            "0   0.5032  \n",
            "1  -3.3223  \n",
            "2  -3.4463  \n",
            "3  -3.4229  \n",
            "4  -3.4368  \n",
            "5   0.4453  \n",
            "6  -3.7705  \n",
            "7  -5.8435  \n",
            "8  -5.4276  \n",
            "9  -1.5758  \n",
            "10  0.7798  \n",
            "11  0.5416  \n",
            "12  0.5497  \n",
            "13  0.5507  \n",
            "14  0.3987  \n"
          ]
        }
      ],
      "source": [
        "summary_data = []\n",
        "\n",
        "for dataset in DATASETS:\n",
        "\n",
        "    budget = ACTIVE_PARAMS[dataset]['budget']\n",
        "    # Passive learning\n",
        "    summary_data.append({\n",
        "        'dataset': dataset,\n",
        "        'method': 'passive',\n",
        "        'budget': budget,\n",
        "        'rmse': test_results['passive'][dataset]['rmse'],\n",
        "        'mae': test_results['passive'][dataset]['mae'],\n",
        "        'r2': test_results['passive'][dataset]['r2'],\n",
        "    })\n",
        "    \n",
        "    # Uncertainty methods\n",
        "    for method in UNCERTAINTY_METHODS:\n",
        "        max_budget = str(budget)\n",
        "        summary_data.append({\n",
        "            'dataset': dataset,\n",
        "            'method': f'uncertainty_{method}',\n",
        "            'budget': budget,\n",
        "            'rmse': test_results['uncertainty'][dataset][method][max_budget]['rmse'],\n",
        "            'mae': test_results['uncertainty'][dataset][method][max_budget]['mae'],\n",
        "            'r2': test_results['uncertainty'][dataset][method][max_budget]['r2'],\n",
        "        })\n",
        "    \n",
        "    # Sensitivity method\n",
        "    max_budget = str(budget)\n",
        "    summary_data.append({\n",
        "        'dataset': dataset,\n",
        "        'method': 'sensitivity',\n",
        "        'budget': budget,\n",
        "        'rmse': test_results['sensitivity'][dataset][max_budget]['rmse'],\n",
        "        'mae': test_results['sensitivity'][dataset][max_budget]['mae'],\n",
        "        'r2': test_results['sensitivity'][dataset][max_budget]['r2'],\n",
        "    })\n",
        "\n",
        "# Convert to DataFrame for nice display\n",
        "df = pd.DataFrame(summary_data)\n",
        "print(df.round(4))\n",
        "\n",
        "# Save summary\n",
        "df.to_csv(os.path.join(SAVE_DIR, 'reg_comparison_summary_test.csv'), index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Machine-Learning-441-fvr4FmDE",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
