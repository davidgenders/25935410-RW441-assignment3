{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from nn.models import OneHiddenMLP\n",
        "from nn.training import train_passive, TrainConfig\n",
        "from nn.evaluation import evaluate_regression\n",
        "from nn.experiments import ActiveConfig, run_active_regression\n",
        "from nn.strategies import uncertainty_sampling, sensitivity_sampling, UncertaintySamplingConfig\n",
        "\n",
        "from typing import Dict\n",
        "\n",
        "SAVE_DIR = os.path.join('..', 'report', 'figures')\n",
        "DATA_DIR = os.path.join('..', 'data')\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "with open(os.path.join(DATA_DIR, 'reg_uncertainty_results.json'), 'r') as f:\n",
        "    unc_results = json.load(f)\n",
        "with open(os.path.join(DATA_DIR, 'reg_sensitivity_results.json'), 'r') as f:\n",
        "    sen_results = json.load(f)\n",
        "with open(os.path.join(DATA_DIR, 'passive_reg_best.json'), 'r') as f:\n",
        "    pas_results = json.load(f)\n",
        "\n",
        "DATASETS = ['diabetes', 'linnerud', 'california']\n",
        "METRICS = ['rmse', 'mae', 'r2']\n",
        "BUDGETS = [200]\n",
        "UNCERTAINTY_METHODS = ['entropy', 'margin', 'least_confidence']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set evaluation functions for regression defined!\n"
          ]
        }
      ],
      "source": [
        "def get_data_splits_regression(dataset: str):\n",
        "    # Load data\n",
        "    if dataset == \"diabetes\":\n",
        "        ds = datasets.load_diabetes()\n",
        "        y = ds.target.astype(np.float32)\n",
        "    elif dataset == \"linnerud\":\n",
        "        ds = datasets.load_linnerud()\n",
        "        y = ds.target[:, 0].astype(np.float32)\n",
        "    elif dataset == \"california\":\n",
        "        ds = datasets.fetch_california_housing()\n",
        "        y = ds.target.astype(np.float32)\n",
        "    \n",
        "    X = ds.data.astype(np.float32)\n",
        "    \n",
        "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "    \n",
        "    return X_train_val, X_test, y_train_val, y_test\n",
        "\n",
        "def evaluate_passive_test_regression(dataset: str, budget: int) -> Dict[str, float]:\n",
        "    # Get best hyperparameters from tuning results\n",
        "    best_cfg = pas_results[dataset]['best_cfg']\n",
        "    lr = best_cfg['lr']\n",
        "    wd = best_cfg['wd']\n",
        "    hidden = best_cfg['hidden']\n",
        "    bs = best_cfg['bs']\n",
        "    \n",
        "    # Get data splits\n",
        "    X_train_val, X_test, y_train_val, y_test = get_data_splits_regression(dataset)\n",
        "    \n",
        "    all_metrics = []\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_val)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    \n",
        "    # Convert to tensors\n",
        "    X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train_val, dtype=torch.float32).unsqueeze(-1)\n",
        "    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(-1)\n",
        "    \n",
        "    # Create datasets\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "    \n",
        "    train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=bs, shuffle=False)\n",
        "    \n",
        "    # Train model\n",
        "    model = OneHiddenMLP(input_dim=X_train_scaled.shape[1], hidden_units=hidden, output_dim=1)\n",
        "    loss_fn = nn.MSELoss()\n",
        "    config = TrainConfig(learning_rate=lr, weight_decay=wd, batch_size=bs, max_epochs=200, patience=20, device='cpu')\n",
        "    \n",
        "    train_passive(model, train_loader, test_loader, loss_fn, config)\n",
        "    \n",
        "    # Evaluate on test set\n",
        "    metrics = evaluate_regression(model, test_loader, device='cpu')\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "def evaluate_active_test_regression(dataset: str, strategy: str, method: str, budget: int) -> Dict[str, float]:\n",
        "    # Get best hyperparameters from tuning \n",
        "    best_cfg = pas_results[dataset]['best_cfg']\n",
        "    lr = best_cfg['lr']\n",
        "    wd = best_cfg['wd']\n",
        "    hidden = best_cfg['hidden']\n",
        "    bs = best_cfg['bs']\n",
        "    init, query = 20, 10\n",
        "\n",
        "    if strategy == 'uncertainty':\n",
        "        # Get hyperparameters tuned for this uncertainty method\n",
        "        best_results = unc_results[dataset][method]\n",
        "        # Extract hyperparameters from the tuned results\n",
        "        # You need to access the actual tuned config, not passive results\n",
        "    elif strategy == 'sensitivity':\n",
        "        # Get hyperparameters tuned for sensitivity\n",
        "        best_results = sen_results[dataset]\n",
        "        # Extract hyperparameters from sensitivity tuning\n",
        "        method = ''  # Only override for sensitivity\n",
        "\n",
        "    # Get data splits\n",
        "    X_train_val, X_test, y_train_val, y_test = get_data_splits_regression(dataset)\n",
        "    \n",
        "    all_metrics = []\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_val)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    \n",
        "    # Convert to tensors\n",
        "    X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train_val, dtype=torch.float32).unsqueeze(-1)\n",
        "    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(-1)\n",
        "    \n",
        "    # Create datasets\n",
        "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "    test_loader = DataLoader(test_dataset, batch_size=bs, shuffle=False)\n",
        "    \n",
        "    # Simulate active learning on the train+val set\n",
        "    train_config = TrainConfig(learning_rate=lr, weight_decay=wd, batch_size=bs, \n",
        "                                max_epochs=200, patience=20, device='cpu')\n",
        "    \n",
        "    # Create initial labeled pool\n",
        "    num_train = X_train_scaled.shape[0]\n",
        "    labeled_indices = torch.randperm(num_train)[:init]\n",
        "    unlabeled_indices = torch.tensor([i for i in range(num_train) if i not in labeled_indices.tolist()], dtype=torch.long)\n",
        "    \n",
        "    x_pool = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "    y_pool = y_train_tensor.clone()\n",
        "    \n",
        "    # Active learning loop\n",
        "    while labeled_indices.numel() < min(budget, num_train):\n",
        "        # Train model on current labeled set\n",
        "        train_subset = TensorDataset(x_pool[labeled_indices], y_pool[labeled_indices])\n",
        "        \n",
        "        train_loader = DataLoader(train_subset, batch_size=bs, shuffle=True)\n",
        "        \n",
        "        model = OneHiddenMLP(input_dim=X_train_scaled.shape[1], hidden_units=hidden, output_dim=1)\n",
        "        loss_fn = nn.MSELoss()\n",
        "        \n",
        "        train_passive(model, train_loader, test_loader, loss_fn, train_config)\n",
        "        \n",
        "        if unlabeled_indices.numel() == 0:\n",
        "            break\n",
        "        \n",
        "        # Query selection\n",
        "        if strategy == 'uncertainty':\n",
        "            sel = uncertainty_sampling(\n",
        "                model,\n",
        "                x_pool[unlabeled_indices].to(train_config.device),\n",
        "                query,\n",
        "                UncertaintySamplingConfig(mode=\"regression\", method=method),\n",
        "            )\n",
        "        elif strategy == 'sensitivity':\n",
        "            sel = sensitivity_sampling(model, x_pool[unlabeled_indices].to(train_config.device), query)\n",
        "        \n",
        "        # Update labeled and unlabeled sets\n",
        "        newly_selected = unlabeled_indices[sel]\n",
        "        labeled_indices = torch.unique(torch.cat([labeled_indices, newly_selected]))\n",
        "        mask = torch.ones_like(unlabeled_indices, dtype=torch.bool)\n",
        "        mask[sel] = False\n",
        "        unlabeled_indices = unlabeled_indices[mask]\n",
        "        \n",
        "        if labeled_indices.numel() >= budget:\n",
        "            break\n",
        "    \n",
        "    # Final evaluation on test set\n",
        "    final_train_subset = TensorDataset(x_pool[labeled_indices], y_pool[labeled_indices])\n",
        "    final_train_loader = DataLoader(final_train_subset, batch_size=bs, shuffle=True)\n",
        "    final_test_loader = DataLoader(test_dataset, batch_size=bs, shuffle=False)\n",
        "    \n",
        "    final_model = OneHiddenMLP(input_dim=X_train_scaled.shape[1], hidden_units=hidden, output_dim=1)\n",
        "    loss_fn = nn.MSELoss()\n",
        "    \n",
        "    train_passive(final_model, final_train_loader, final_test_loader, loss_fn, train_config)\n",
        "    \n",
        "    # Evaluate on test set\n",
        "    metrics = evaluate_regression(final_model, final_test_loader, device='cpu')\n",
        "    \n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting test set evaluation for regression...\n",
            "This will evaluate all methods on the held-out test sets that were never seen during hyperparameter tuning.\n",
            "\n",
            "Evaluating passive learning on test sets...\n",
            "  diabetes...\n",
            "  linnerud...\n",
            "  california...\n",
            "\n",
            "Evaluating uncertainty-based active learning on test sets...\n",
            "  diabetes - entropy...\n",
            "  diabetes - margin...\n",
            "  diabetes - least_confidence...\n",
            "  linnerud - entropy...\n",
            "  linnerud - margin...\n",
            "  linnerud - least_confidence...\n",
            "  california - entropy...\n",
            "  california - margin...\n",
            "  california - least_confidence...\n",
            "\n",
            "Evaluating sensitivity-based active learning on test sets...\n",
            "  diabetes...\n",
            "  linnerud...\n",
            "  california...\n",
            "\n",
            "Test set evaluation for regression completed!\n",
            "Results stored in test_results dictionary.\n"
          ]
        }
      ],
      "source": [
        "test_results = {\n",
        "    'passive': {},\n",
        "    'uncertainty': {},\n",
        "    'sensitivity': {}\n",
        "}\n",
        "\n",
        "# Evaluate passive learning on test set\n",
        "for dataset in DATASETS:\n",
        "    print(f\"Running passive on {dataset}\")\n",
        "    test_results['passive'][dataset] = evaluate_passive_test_regression(dataset, max(BUDGETS))\n",
        "\n",
        "# Evaluate uncertainty-based active learning on test set\n",
        "for dataset in DATASETS:\n",
        "    test_results['uncertainty'][dataset] = {}\n",
        "    for method in UNCERTAINTY_METHODS:\n",
        "        print(f\"Running uncertainty on {dataset} - {method}\")\n",
        "        test_results['uncertainty'][dataset][method] = {}\n",
        "        for budget in BUDGETS:\n",
        "            test_results['uncertainty'][dataset][method][str(budget)] = evaluate_active_test_regression(dataset, 'uncertainty', method, budget)\n",
        "\n",
        "# Evaluate sensitivity-based active learning on test set\n",
        "for dataset in DATASETS:\n",
        "    print(f\"Running sensitivity on {dataset}...\")\n",
        "    test_results['sensitivity'][dataset] = {}\n",
        "    for budget in BUDGETS:\n",
        "        test_results['sensitivity'][dataset][str(budget)] = evaluate_active_test_regression(dataset, 'sensitivity', '', budget)\n",
        "\n",
        "print(\"Done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Comparing strategies for diabetes (TEST SET) ===\n",
            "Saved diabetes rmse comparison (TEST SET)\n",
            "Saved diabetes mae comparison (TEST SET)\n",
            "Saved diabetes r2 comparison (TEST SET)\n",
            "\n",
            "=== Comparing strategies for linnerud (TEST SET) ===\n",
            "Saved linnerud rmse comparison (TEST SET)\n",
            "Saved linnerud mae comparison (TEST SET)\n",
            "Saved linnerud r2 comparison (TEST SET)\n",
            "\n",
            "=== Comparing strategies for california (TEST SET) ===\n",
            "Saved california rmse comparison (TEST SET)\n",
            "Saved california mae comparison (TEST SET)\n",
            "Saved california r2 comparison (TEST SET)\n"
          ]
        }
      ],
      "source": [
        "for dataset in DATASETS:\n",
        "    for metric in METRICS:\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        \n",
        "        # Plot uncertainty methods\n",
        "        for method in UNCERTAINTY_METHODS:\n",
        "            result = [test_results['uncertainty'][dataset][method][str(b)][f'{metric}'] for b in BUDGETS]\n",
        "            plt.plot(BUDGETS, result, marker='o', label=f'uncertainty_{method}', linewidth=2)\n",
        "        \n",
        "        # Plot sensitivity method\n",
        "        result = [test_results['sensitivity'][dataset][str(b)][f'{metric}'] for b in BUDGETS]\n",
        "        plt.plot(BUDGETS, result, marker='s', label='sensitivity', linewidth=2)\n",
        "        \n",
        "        # Plot passive baseline as horizontal line\n",
        "        baseline = test_results['passive'][dataset][f'{metric}']\n",
        "        plt.axhline(baseline, color='k', linestyle='--', label='passive_best', linewidth=2)\n",
        "        \n",
        "        plt.xlabel('Labeled budget (max_labels)')\n",
        "        plt.ylabel(metric)\n",
        "        plt.title(f'{dataset}: Strategies Comparison ({metric}) - TEST SET EVALUATION')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(SAVE_DIR, f'reg_{dataset}_comparison_{metric}_test.png'), dpi=200, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        \n",
        "        print(f\"Saved {dataset} {metric} comparison (TEST SET)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Summary Table (TEST SET EVALUATION) ===\n",
            "       dataset                        method  budget     rmse      mae      r2\n",
            "0     diabetes                       passive     200  51.6864  40.6610  0.4958\n",
            "1     diabetes           uncertainty_entropy     200  54.5341  43.4015  0.4387\n",
            "2     diabetes            uncertainty_margin     200  55.9546  45.1058  0.4091\n",
            "3     diabetes  uncertainty_least_confidence     200  53.7882  43.2867  0.4539\n",
            "4     diabetes                   sensitivity     200  52.8503  42.2713  0.4728\n",
            "5     linnerud                       passive     200  55.3287  46.7521 -9.8869\n",
            "6     linnerud           uncertainty_entropy     200  55.3851  42.8853 -9.9091\n",
            "7     linnerud            uncertainty_margin     200  44.4944  36.0868 -6.0407\n",
            "8     linnerud  uncertainty_least_confidence     200  49.9238  40.5934 -7.8638\n",
            "9     linnerud                   sensitivity     200  50.3240  40.4513 -8.0065\n",
            "10  california                       passive     200   0.5317   0.3603  0.7842\n",
            "11  california           uncertainty_entropy     200   0.9086   0.6137  0.3701\n",
            "12  california            uncertainty_margin     200   0.9268   0.6729  0.3445\n",
            "13  california  uncertainty_least_confidence     200   0.7896   0.5749  0.5242\n",
            "14  california                   sensitivity     200   0.8214   0.5733  0.4851\n",
            "\n",
            "Saved TEST SET comparison summary to ../report/figures\n",
            "All TEST SET comparison figures and summary saved!\n"
          ]
        }
      ],
      "source": [
        "summary_data = []\n",
        "\n",
        "for dataset in DATASETS:\n",
        "    # Passive learning\n",
        "    summary_data.append({\n",
        "        'dataset': dataset,\n",
        "        'method': 'passive',\n",
        "        'budget': max(BUDGETS),\n",
        "        'rmse': test_results['passive'][dataset]['rmse'],\n",
        "        'mae': test_results['passive'][dataset]['mae'],\n",
        "        'r2': test_results['passive'][dataset]['r2'],\n",
        "    })\n",
        "    \n",
        "    # Uncertainty methods\n",
        "    for method in UNCERTAINTY_METHODS:\n",
        "        max_budget = str(max(BUDGETS))\n",
        "        summary_data.append({\n",
        "            'dataset': dataset,\n",
        "            'method': f'uncertainty_{method}',\n",
        "            'budget': max(BUDGETS),\n",
        "            'rmse': test_results['uncertainty'][dataset][method][max_budget]['rmse'],\n",
        "            'mae': test_results['uncertainty'][dataset][method][max_budget]['mae'],\n",
        "            'r2': test_results['uncertainty'][dataset][method][max_budget]['r2'],\n",
        "        })\n",
        "    \n",
        "    # Sensitivity method\n",
        "    max_budget = str(max(BUDGETS))\n",
        "    summary_data.append({\n",
        "        'dataset': dataset,\n",
        "        'method': 'sensitivity',\n",
        "        'budget': max(BUDGETS),\n",
        "        'rmse': test_results['sensitivity'][dataset][max_budget]['rmse'],\n",
        "        'mae': test_results['sensitivity'][dataset][max_budget]['mae'],\n",
        "        'r2': test_results['sensitivity'][dataset][max_budget]['r2'],\n",
        "    })\n",
        "\n",
        "# Convert to DataFrame for nice display\n",
        "df = pd.DataFrame(summary_data)\n",
        "print(df.round(4))\n",
        "\n",
        "# Save summary\n",
        "df.to_csv(os.path.join(SAVE_DIR, 'reg_comparison_summary_test.csv'), index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Machine-Learning-441-fvr4FmDE",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
