{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Active Learning Exploration - Classification (Uncertainty) with Cross-Validation\n",
        "import os\n",
        "import json\n",
        "import itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from typing import Dict, List\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "from alnn.experiments import ActiveConfig, run_active_classification\n",
        "from alnn.training import TrainConfig\n",
        "\n",
        "SAVE_DIR = os.path.join('..', 'report', 'figures')\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "DATASETS = ['iris', 'wine', 'breast_cancer']\n",
        "METHODS = ['entropy', 'margin', 'least_confidence']\n",
        "BUDGETS = [40, 80, 120, 160, 200]\n",
        "N_TRIALS = 5  # Number of random seeds for each config\n",
        "N_FOLDS = 3  # Number of CV folds (reduced for active learning)\n",
        "\n",
        "# Tuning grids\n",
        "LRS = [1e-3, 3e-3, 1e-2]\n",
        "WDS = [0.0, 1e-5, 1e-4]\n",
        "HIDDENS = [32, 64, 128]\n",
        "BSS = [32, 64]\n",
        "INITS = [10, 20, 40]\n",
        "QUERIES = [5, 10, 20]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tune_hparams(dataset: str, method: str, tune_budget: int) -> tuple:\n",
        "    \"\"\"Tune hyperparameters using cross-validation.\"\"\"\n",
        "    best_acc = -float('inf')\n",
        "    best_config = None\n",
        "    \n",
        "    total_configs = len(LRS) * len(WDS) * len(HIDDENS) * len(BSS) * len(INITS) * len(QUERIES)\n",
        "    \n",
        "    # Load checkpoint if exists\n",
        "    checkpoint_file = os.path.join(SAVE_DIR, f'cls_uncertainty_{dataset}_{method}_checkpoint.json')\n",
        "    if os.path.exists(checkpoint_file):\n",
        "        with open(checkpoint_file, 'r') as f:\n",
        "            checkpoint = json.load(f)\n",
        "        print(f\"Resuming hyperparameter tuning from checkpoint: {checkpoint['completed_configs']} configs completed\")\n",
        "        best_acc = checkpoint.get('best_acc', -float('inf'))\n",
        "        best_config = checkpoint.get('best_config', None)\n",
        "        completed_configs = checkpoint['completed_configs']\n",
        "    else:\n",
        "        checkpoint = {'completed_configs': 0, 'best_acc': -float('inf'), 'best_config': None}\n",
        "        completed_configs = 0\n",
        "        print(\"Starting fresh hyperparameter tuning\")\n",
        "    \n",
        "    # Create progress bar\n",
        "    pbar = tqdm(total=total_configs, desc=f\"Tuning {dataset}-{method}\", \n",
        "                initial=completed_configs, position=0, leave=True)\n",
        "    \n",
        "    config_idx = completed_configs\n",
        "    for lr, wd, hidden, bs, init, query in itertools.product(LRS, WDS, HIDDENS, BSS, INITS, QUERIES):\n",
        "        if config_idx < completed_configs:\n",
        "            config_idx += 1\n",
        "            pbar.update(1)\n",
        "            continue\n",
        "            \n",
        "        print(f'Tuning config {config_idx+1}/{total_configs}: lr={lr}, wd={wd}, hidden={hidden}, bs={bs}, init={init}, query={query}')\n",
        "        \n",
        "        # Evaluate this configuration\n",
        "        metrics = []\n",
        "        for seed in range(N_TRIALS):\n",
        "            torch.manual_seed(42 + seed)\n",
        "            tcfg = TrainConfig(learning_rate=lr, weight_decay=wd, batch_size=bs, max_epochs=200, patience=20, device='cpu')\n",
        "            acfg = ActiveConfig(initial_labeled=init, query_batch=query, max_labels=tune_budget, device='cpu')\n",
        "            res = run_active_classification(dataset_name=dataset, strategy='uncertainty', uncertainty_method=method, \n",
        "                                          hidden_units=hidden, train_config=tcfg, active_config=acfg)\n",
        "            metrics.append(res['accuracy'])\n",
        "        \n",
        "        avg_acc = np.mean(metrics)\n",
        "        \n",
        "        if avg_acc > best_acc:\n",
        "            best_acc = avg_acc\n",
        "            best_config = (TrainConfig(learning_rate=lr, weight_decay=wd, batch_size=bs, max_epochs=200, patience=20, device='cpu'),\n",
        "                          ActiveConfig(initial_labeled=init, query_batch=query, max_labels=tune_budget, device='cpu'),\n",
        "                          hidden)\n",
        "        \n",
        "        # Update progress bar\n",
        "        pbar.update(1)\n",
        "        pbar.set_postfix({'best_acc': f\"{best_acc:.4f}\"})\n",
        "        \n",
        "        # Save checkpoint after each config\n",
        "        checkpoint['completed_configs'] = config_idx + 1\n",
        "        checkpoint['best_acc'] = best_acc\n",
        "        checkpoint['best_config'] = best_config\n",
        "        with open(checkpoint_file, 'w') as f:\n",
        "            json.dump(checkpoint, f, indent=2)\n",
        "        \n",
        "        config_idx += 1\n",
        "    \n",
        "    pbar.close()\n",
        "    \n",
        "    # Clean up checkpoint file\n",
        "    if os.path.exists(checkpoint_file):\n",
        "        os.remove(checkpoint_file)\n",
        "    \n",
        "    print(f\"Best config for {dataset}-{method}: accuracy={best_acc:.4f}\")\n",
        "    return best_config\n",
        "\n",
        "\n",
        "def evaluate_curve(dataset: str, method: str, budgets: List[int]) -> Dict[int, Dict[str, float]]:\n",
        "    \"\"\"Evaluate active learning curve using best hyperparameters.\"\"\"\n",
        "    tune_budget = sorted(budgets)[len(budgets)//2]\n",
        "    tcfg, acfg_base, hidden_units = tune_hparams(dataset, method, tune_budget)\n",
        "\n",
        "    # Load checkpoint if exists\n",
        "    checkpoint_file = os.path.join(SAVE_DIR, f'cls_uncertainty_{dataset}_{method}_curve_checkpoint.json')\n",
        "    if os.path.exists(checkpoint_file):\n",
        "        with open(checkpoint_file, 'r') as f:\n",
        "            checkpoint = json.load(f)\n",
        "        print(f\"Resuming curve evaluation from checkpoint: {len(checkpoint['results'])} budgets completed\")\n",
        "        results = checkpoint['results']\n",
        "    else:\n",
        "        checkpoint = {'results': {}}\n",
        "        results = {}\n",
        "        print(\"Starting fresh curve evaluation\")\n",
        "    \n",
        "    # Create progress bar\n",
        "    pbar = tqdm(total=len(budgets), desc=f\"Curve {dataset}-{method}\", \n",
        "                initial=len(results), position=0, leave=True)\n",
        "    \n",
        "    for max_labels in budgets:\n",
        "        if str(max_labels) in results:\n",
        "            pbar.update(1)\n",
        "            continue\n",
        "            \n",
        "        print(f'Evaluating {dataset}-{method} at budget {max_labels}')\n",
        "        metrics = []\n",
        "        for seed in range(N_TRIALS):\n",
        "            torch.manual_seed(42 + seed)\n",
        "            acfg = ActiveConfig(initial_labeled=acfg_base.initial_labeled, query_batch=acfg_base.query_batch, max_labels=max_labels, device=acfg_base.device)\n",
        "            res = run_active_classification(dataset_name=dataset, strategy='uncertainty', uncertainty_method=method, hidden_units=hidden_units, train_config=tcfg, active_config=acfg)\n",
        "            metrics.append(res)\n",
        "        \n",
        "        keys = metrics[0].keys()\n",
        "        results[str(max_labels)] = {f'{k}_mean': float(np.mean([m[k] for m in metrics])) for k in keys}\n",
        "        results[str(max_labels)].update({f'{k}_std': float(np.std([m[k] for m in metrics], ddof=1)) for k in keys})\n",
        "        \n",
        "        # Update progress bar\n",
        "        pbar.update(1)\n",
        "        \n",
        "        # Save checkpoint after each budget\n",
        "        checkpoint['results'] = results\n",
        "        with open(checkpoint_file, 'w') as f:\n",
        "            json.dump(checkpoint, f, indent=2)\n",
        "    \n",
        "    pbar.close()\n",
        "    \n",
        "    # Clean up checkpoint file\n",
        "    if os.path.exists(checkpoint_file):\n",
        "        os.remove(checkpoint_file)\n",
        "    \n",
        "    return {int(k): v for k, v in results.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load checkpoint if exists\n",
        "checkpoint_file = os.path.join(SAVE_DIR, 'cls_uncertainty_main_checkpoint.json')\n",
        "if os.path.exists(checkpoint_file):\n",
        "    with open(checkpoint_file, 'r') as f:\n",
        "        checkpoint = json.load(f)\n",
        "    print(f\"Resuming from checkpoint: {checkpoint['completed_datasets']} datasets completed\")\n",
        "    all_results = checkpoint.get('results', {})\n",
        "else:\n",
        "    checkpoint = {'completed_datasets': 0, 'results': {}}\n",
        "    all_results = {}\n",
        "    print(\"Starting fresh run\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for dataset in DATASETS:\n",
        "    if dataset in all_results:\n",
        "        print(f\"\\n=== Skipping {dataset} (already completed) ===\")\n",
        "        continue\n",
        "        \n",
        "    print(f\"\\n=== Processing {dataset} ===\")\n",
        "    all_results[dataset] = {}\n",
        "    \n",
        "    for method in METHODS:\n",
        "        print(f\"\\n--- Method: {method} ---\")\n",
        "        curve = evaluate_curve(dataset, method, BUDGETS)\n",
        "        all_results[dataset][method] = curve\n",
        "        \n",
        "        # Plot curves\n",
        "        for metric in ['accuracy', 'f1_macro']:\n",
        "            budgets = sorted(curve.keys())\n",
        "            means = [curve[b][f'{metric}_mean'] for b in budgets]\n",
        "            stds = [curve[b][f'{metric}_std'] for b in budgets]\n",
        "            plt.figure(figsize=(8, 5))\n",
        "            plt.plot(budgets, means, marker='o', label=method, linewidth=2)\n",
        "            plt.fill_between(budgets, np.array(means)-np.array(stds), np.array(means)+np.array(stds), alpha=0.2)\n",
        "            plt.xlabel('Labeled budget (max_labels)')\n",
        "            plt.ylabel(metric)\n",
        "            plt.title(f'{dataset} - {method} ({metric}) - CV + Multiple Trials')\n",
        "            plt.grid(True, alpha=0.3)\n",
        "            plt.legend()\n",
        "            fname = f'cls_{dataset}_uncertainty_{method}_{metric}.png'\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(SAVE_DIR, fname), dpi=200)\n",
        "            plt.close()\n",
        "    \n",
        "    # Save checkpoint after each dataset\n",
        "    checkpoint['completed_datasets'] += 1\n",
        "    checkpoint['results'] = all_results\n",
        "    with open(checkpoint_file, 'w') as f:\n",
        "        json.dump(checkpoint, f, indent=2)\n",
        "\n",
        "# Save final results\n",
        "with open(os.path.join(SAVE_DIR, 'cls_uncertainty_results.json'), 'w') as f:\n",
        "    json.dump(all_results, f, indent=2)\n",
        "\n",
        "# Clean up checkpoint file\n",
        "if os.path.exists(checkpoint_file):\n",
        "    os.remove(checkpoint_file)\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(f\"\\nTotal time: {total_time/3600:.2f} hours\")\n",
        "print(f'\\nSaved figures and results to {SAVE_DIR}')\n",
        "print(f'Used {N_TRIALS} trials Ã— {N_FOLDS} folds = {N_TRIALS * N_FOLDS} evaluations per config')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot summary comparison\n",
        "for dataset in DATASETS:\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    \n",
        "    for metric in ['accuracy', 'f1_macro']:\n",
        "        plt.subplot(2, 1, 1 if metric == 'accuracy' else 2)\n",
        "        \n",
        "        for method in METHODS:\n",
        "            if dataset in all_results and method in all_results[dataset]:\n",
        "                curve = all_results[dataset][method]\n",
        "                budgets = sorted(curve.keys())\n",
        "                means = [curve[b][f'{metric}_mean'] for b in budgets]\n",
        "                stds = [curve[b][f'{metric}_std'] for b in budgets]\n",
        "                plt.plot(budgets, means, marker='o', label=f'{method}', linewidth=2)\n",
        "                plt.fill_between(budgets, np.array(means)-np.array(stds), np.array(means)+np.array(stds), alpha=0.2)\n",
        "        \n",
        "        plt.xlabel('Labeled budget (max_labels)')\n",
        "        plt.ylabel(metric)\n",
        "        plt.title(f'{dataset} - Uncertainty Methods Comparison ({metric})')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(SAVE_DIR, f'cls_{dataset}_uncertainty_comparison.png'), dpi=200)\n",
        "    plt.show()\n",
        "\n",
        "print('\\nAll comparison plots saved!')\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
